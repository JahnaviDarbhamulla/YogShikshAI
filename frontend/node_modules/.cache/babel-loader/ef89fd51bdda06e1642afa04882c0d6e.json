{"ast":null,"code":"import _classCallCheck from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _inherits from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/inherits\";\nimport _createSuper from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/createSuper\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { deserializeKerasObject } from './utils/generic_utils';\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\n\nexport var Activation = /*#__PURE__*/function (_serialization$Serial) {\n  _inherits(Activation, _serialization$Serial);\n\n  var _super = _createSuper(Activation);\n\n  function Activation() {\n    _classCallCheck(this, Activation);\n\n    return _super.apply(this, arguments);\n  }\n\n  _createClass(Activation, [{\n    key: \"getConfig\",\n    value: function getConfig() {\n      return {};\n    }\n  }]);\n\n  return Activation;\n}(serialization.Serializable);\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\n\nexport var Elu = /*#__PURE__*/function (_Activation) {\n  _inherits(Elu, _Activation);\n\n  var _super2 = _createSuper(Elu);\n\n  function Elu() {\n    _classCallCheck(this, Elu);\n\n    return _super2.apply(this, arguments);\n  }\n\n  _createClass(Elu, [{\n    key: \"apply\",\n    value:\n    /**\n     * Calculate the activation function.\n     *\n     * @param x: Input.\n     * @param alpha: Scaling factor the negative section.\n     * @return Output of the ELU activation.\n     */\n    function apply(x) {\n      var alpha = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n      return K.elu(x, alpha);\n    }\n  }]);\n\n  return Elu;\n}(Activation);\n/** @nocollapse */\n\nElu.className = 'elu';\nserialization.registerClass(Elu);\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\n\nexport var Selu = /*#__PURE__*/function (_Activation2) {\n  _inherits(Selu, _Activation2);\n\n  var _super3 = _createSuper(Selu);\n\n  function Selu() {\n    _classCallCheck(this, Selu);\n\n    return _super3.apply(this, arguments);\n  }\n\n  _createClass(Selu, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return tfc.selu(x);\n    }\n  }]);\n\n  return Selu;\n}(Activation);\n/** @nocollapse */\n\nSelu.className = 'selu';\nserialization.registerClass(Selu);\n/**\n *  Rectified linear unit\n */\n\nexport var Relu = /*#__PURE__*/function (_Activation3) {\n  _inherits(Relu, _Activation3);\n\n  var _super4 = _createSuper(Relu);\n\n  function Relu() {\n    _classCallCheck(this, Relu);\n\n    return _super4.apply(this, arguments);\n  }\n\n  _createClass(Relu, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return tfc.relu(x);\n    }\n  }]);\n\n  return Relu;\n}(Activation);\n/** @nocollapse */\n\nRelu.className = 'relu';\nserialization.registerClass(Relu);\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\n\nexport var Relu6 = /*#__PURE__*/function (_Activation4) {\n  _inherits(Relu6, _Activation4);\n\n  var _super5 = _createSuper(Relu6);\n\n  function Relu6() {\n    _classCallCheck(this, Relu6);\n\n    return _super5.apply(this, arguments);\n  }\n\n  _createClass(Relu6, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return tidy(function () {\n        return tfc.minimum(6.0, tfc.relu(x));\n      });\n    }\n  }]);\n\n  return Relu6;\n}(Activation);\n/** @nocollapse */\n\nRelu6.className = 'relu6';\nserialization.registerClass(Relu6); //* Linear activation (no-op) */\n\nexport var Linear = /*#__PURE__*/function (_Activation5) {\n  _inherits(Linear, _Activation5);\n\n  var _super6 = _createSuper(Linear);\n\n  function Linear() {\n    _classCallCheck(this, Linear);\n\n    return _super6.apply(this, arguments);\n  }\n\n  _createClass(Linear, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return x;\n    }\n  }]);\n\n  return Linear;\n}(Activation);\n/** @nocollapse */\n\nLinear.className = 'linear';\nserialization.registerClass(Linear);\n/**\n * Sigmoid activation function.\n */\n\nexport var Sigmoid = /*#__PURE__*/function (_Activation6) {\n  _inherits(Sigmoid, _Activation6);\n\n  var _super7 = _createSuper(Sigmoid);\n\n  function Sigmoid() {\n    _classCallCheck(this, Sigmoid);\n\n    return _super7.apply(this, arguments);\n  }\n\n  _createClass(Sigmoid, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return tfc.sigmoid(x);\n    }\n  }]);\n\n  return Sigmoid;\n}(Activation);\n/** @nocollapse */\n\nSigmoid.className = 'sigmoid';\nserialization.registerClass(Sigmoid);\n/**\n * Segment-wise linear approximation of sigmoid.\n */\n\nexport var HardSigmoid = /*#__PURE__*/function (_Activation7) {\n  _inherits(HardSigmoid, _Activation7);\n\n  var _super8 = _createSuper(HardSigmoid);\n\n  function HardSigmoid() {\n    _classCallCheck(this, HardSigmoid);\n\n    return _super8.apply(this, arguments);\n  }\n\n  _createClass(HardSigmoid, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return K.hardSigmoid(x);\n    }\n  }]);\n\n  return HardSigmoid;\n}(Activation);\n/** @nocollapse */\n\nHardSigmoid.className = 'hardSigmoid';\nserialization.registerClass(HardSigmoid);\n/**\n * Softplus activation function.\n */\n\nexport var Softplus = /*#__PURE__*/function (_Activation8) {\n  _inherits(Softplus, _Activation8);\n\n  var _super9 = _createSuper(Softplus);\n\n  function Softplus() {\n    _classCallCheck(this, Softplus);\n\n    return _super9.apply(this, arguments);\n  }\n\n  _createClass(Softplus, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return tfc.softplus(x);\n    }\n  }]);\n\n  return Softplus;\n}(Activation);\n/** @nocollapse */\n\nSoftplus.className = 'softplus';\nserialization.registerClass(Softplus);\n/**\n * Softsign activation function.\n */\n\nexport var Softsign = /*#__PURE__*/function (_Activation9) {\n  _inherits(Softsign, _Activation9);\n\n  var _super10 = _createSuper(Softsign);\n\n  function Softsign() {\n    _classCallCheck(this, Softsign);\n\n    return _super10.apply(this, arguments);\n  }\n\n  _createClass(Softsign, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return K.softsign(x);\n    }\n  }]);\n\n  return Softsign;\n}(Activation);\n/** @nocollapse */\n\nSoftsign.className = 'softsign';\nserialization.registerClass(Softsign);\n/**\n * Hyperbolic tangent function.\n */\n\nexport var Tanh = /*#__PURE__*/function (_Activation10) {\n  _inherits(Tanh, _Activation10);\n\n  var _super11 = _createSuper(Tanh);\n\n  function Tanh() {\n    _classCallCheck(this, Tanh);\n\n    return _super11.apply(this, arguments);\n  }\n\n  _createClass(Tanh, [{\n    key: \"apply\",\n    value: function apply(x) {\n      return tfc.tanh(x);\n    }\n  }]);\n\n  return Tanh;\n}(Activation);\n/** @nocollapse */\n\nTanh.className = 'tanh';\nserialization.registerClass(Tanh);\n/**\n * Softmax activation function\n */\n\nexport var Softmax = /*#__PURE__*/function (_Activation11) {\n  _inherits(Softmax, _Activation11);\n\n  var _super12 = _createSuper(Softmax);\n\n  function Softmax() {\n    _classCallCheck(this, Softmax);\n\n    return _super12.apply(this, arguments);\n  }\n\n  _createClass(Softmax, [{\n    key: \"apply\",\n    value:\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    function apply(x) {\n      var axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n      return tfc.softmax(x, axis);\n    }\n  }]);\n\n  return Softmax;\n}(Activation);\n/** @nocollapse */\n\nSoftmax.className = 'softmax';\nserialization.registerClass(Softmax);\n/**\n * Log softmax activation function\n */\n\nexport var LogSoftmax = /*#__PURE__*/function (_Activation12) {\n  _inherits(LogSoftmax, _Activation12);\n\n  var _super13 = _createSuper(LogSoftmax);\n\n  function LogSoftmax() {\n    _classCallCheck(this, LogSoftmax);\n\n    return _super13.apply(this, arguments);\n  }\n\n  _createClass(LogSoftmax, [{\n    key: \"apply\",\n    value:\n    /**\n     * Calculate the activation function of log softmax:\n     * log( exp(x_i) / sum(exp(x)) )\n     *\n     * @param x Tensor.\n     * @param axis Integer, axis along which the softmax normalization is applied.\n     * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n     * an error.\n     *\n     * @returns a Tensor of the same shape as x\n     *\n     * @throws ValueError: In case `dim(x) < 2`.\n     */\n    function apply(x) {\n      var axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n      return tfc.logSoftmax(x, axis);\n    }\n  }]);\n\n  return LogSoftmax;\n}(Activation);\n/** @nocollapse */\n\nLogSoftmax.className = 'logSoftmax';\nserialization.registerClass(LogSoftmax);\n/**\n * Swish activation function\n */\n\nexport var Swish = /*#__PURE__*/function (_Activation13) {\n  _inherits(Swish, _Activation13);\n\n  var _super14 = _createSuper(Swish);\n\n  function Swish() {\n    _classCallCheck(this, Swish);\n\n    return _super14.apply(this, arguments);\n  }\n\n  _createClass(Swish, [{\n    key: \"apply\",\n    value:\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @param alpha Scaling factor for the sigmoid function.\n     * @returns a Tensor of the same shape as x\n     */\n    function apply(x) {\n      var alpha = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n      return tidy(function () {\n        return tfc.mul(tfc.sigmoid(tfc.mul(x, alpha)), x);\n      });\n    }\n  }]);\n\n  return Swish;\n}(Activation);\n/** @nocollapse */\n\nSwish.className = 'swish';\nserialization.registerClass(Swish);\n/**\n * Mish activation function\n */\n\nexport var Mish = /*#__PURE__*/function (_Activation14) {\n  _inherits(Mish, _Activation14);\n\n  var _super15 = _createSuper(Mish);\n\n  function Mish() {\n    _classCallCheck(this, Mish);\n\n    return _super15.apply(this, arguments);\n  }\n\n  _createClass(Mish, [{\n    key: \"apply\",\n    value:\n    /**\n     * Calculate the activation function.\n     *\n     * @param x Tensor.\n     * @returns a Tensor of the same shape as x\n     */\n    function apply(x) {\n      return tidy(function () {\n        return tfc.mul(x, tfc.tanh(tfc.softplus(x)));\n      });\n    }\n  }]);\n\n  return Mish;\n}(Activation);\n/** @nocollapse */\n\nMish.className = 'mish';\nserialization.registerClass(Mish);\nexport function serializeActivation(activation) {\n  return activation.getClassName();\n}\nexport function deserializeActivation(config) {\n  var customObjects = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n  return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexport function getActivation(identifier) {\n  if (identifier == null) {\n    var config = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n\n  if (typeof identifier === 'string') {\n    var _config = {};\n    _config['className'] = identifier;\n    _config['config'] = {};\n    return deserializeActivation(_config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}","map":{"version":3,"sources":["../../../../../tfjs-layers/src/activations.ts"],"names":[],"mappings":";;;;;AAAA;;;;;;;;AAQG;AAEH;AACA,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,aAAR,EAA+B,IAA/B,QAA0C,uBAA1C;AACA,OAAO,KAAK,CAAZ,MAAmB,wBAAnB;AAEA,SAAQ,sBAAR,QAAqC,uBAArC;AAEA;;;;;;AAMG;;AACH,WAAsB,UAAtB;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAEE,qBAAS;AACP,aAAO,EAAP;AACD;AAJH;;AAAA;AAAA,EAAyC,aAAa,CAAC,YAAvD;AAOA;;;AAGG;;AACH,WAAa,GAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAGE;;;;;;AAMG;AACH,mBAAM,CAAN,EAA0B;AAAA,UAAT,KAAS,uEAAD,CAAC;AACxB,aAAO,CAAC,CAAC,GAAF,CAAM,CAAN,EAAS,KAAT,CAAP;AACD;AAZH;;AAAA;AAAA,EAAyB,UAAzB;AACE;;AACgB,GAAA,CAAA,SAAA,GAAY,KAAZ;AAYlB,aAAa,CAAC,aAAd,CAA4B,GAA5B;AAEA;;;;;;AAMG;;AACH,WAAa,IAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,GAAG,CAAC,IAAJ,CAAS,CAAT,CAAP;AACD;AALH;;AAAA;AAAA,EAA0B,UAA1B;AACE;;AACgB,IAAA,CAAA,SAAA,GAAY,MAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,IAA5B;AAEA;;AAEG;;AACH,WAAa,IAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,GAAG,CAAC,IAAJ,CAAS,CAAT,CAAP;AACD;AALH;;AAAA;AAAA,EAA0B,UAA1B;AACE;;AACgB,IAAA,CAAA,SAAA,GAAY,MAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,IAA5B;AAEA;;AAEG;;AACH,WAAa,KAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,IAAI,CAAC;AAAA,eAAM,GAAG,CAAC,OAAJ,CAAY,GAAZ,EAAiB,GAAG,CAAC,IAAJ,CAAS,CAAT,CAAjB,CAAN;AAAA,OAAD,CAAX;AACD;AALH;;AAAA;AAAA,EAA2B,UAA3B;AACE;;AACgB,KAAA,CAAA,SAAA,GAAY,OAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,KAA5B,E,CAEA;;AACA,WAAa,MAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,CAAP;AACD;AALH;;AAAA;AAAA,EAA4B,UAA5B;AACE;;AACgB,MAAA,CAAA,SAAA,GAAY,QAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,MAA5B;AAEA;;AAEG;;AACH,WAAa,OAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,GAAG,CAAC,OAAJ,CAAY,CAAZ,CAAP;AACD;AALH;;AAAA;AAAA,EAA6B,UAA7B;AACE;;AACgB,OAAA,CAAA,SAAA,GAAY,SAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,OAA5B;AAEA;;AAEG;;AACH,WAAa,WAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,CAAC,CAAC,WAAF,CAAc,CAAd,CAAP;AACD;AALH;;AAAA;AAAA,EAAiC,UAAjC;AACE;;AACgB,WAAA,CAAA,SAAA,GAAY,aAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,WAA5B;AAEA;;AAEG;;AACH,WAAa,QAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,GAAG,CAAC,QAAJ,CAAa,CAAb,CAAP;AACD;AALH;;AAAA;AAAA,EAA8B,UAA9B;AACE;;AACgB,QAAA,CAAA,SAAA,GAAY,UAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,QAA5B;AAEA;;AAEG;;AACH,WAAa,QAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,CAAC,CAAC,QAAF,CAAW,CAAX,CAAP;AACD;AALH;;AAAA;AAAA,EAA8B,UAA9B;AACE;;AACgB,QAAA,CAAA,SAAA,GAAY,UAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,QAA5B;AAEA;;AAEG;;AACH,WAAa,IAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA,WAGE,eAAM,CAAN,EAAe;AACb,aAAO,GAAG,CAAC,IAAJ,CAAS,CAAT,CAAP;AACD;AALH;;AAAA;AAAA,EAA0B,UAA1B;AACE;;AACgB,IAAA,CAAA,SAAA,GAAY,MAAZ;AAKlB,aAAa,CAAC,aAAd,CAA4B,IAA5B;AAEA;;AAEG;;AACH,WAAa,OAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAGE;;;;;;;;;;;AAWG;AACH,mBAAM,CAAN,EAAoC;AAAA,UAAnB,IAAmB,uEAAH,CAAC,CAAE;AAClC,aAAO,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,IAAf,CAAP;AACD;AAjBH;;AAAA;AAAA,EAA6B,UAA7B;AACE;;AACgB,OAAA,CAAA,SAAA,GAAY,SAAZ;AAiBlB,aAAa,CAAC,aAAd,CAA4B,OAA5B;AAEA;;AAEG;;AACH,WAAa,UAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAGE;;;;;;;;;;;;AAYG;AACH,mBAAM,CAAN,EAAoC;AAAA,UAAnB,IAAmB,uEAAH,CAAC,CAAE;AAClC,aAAO,GAAG,CAAC,UAAJ,CAAe,CAAf,EAAkB,IAAlB,CAAP;AACD;AAlBH;;AAAA;AAAA,EAAgC,UAAhC;AACE;;AACgB,UAAA,CAAA,SAAA,GAAY,YAAZ;AAkBlB,aAAa,CAAC,aAAd,CAA4B,UAA5B;AAEA;;AAEG;;AACH,WAAa,KAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAGE;;;;;;AAMG;AACH,mBAAM,CAAN,EAA0B;AAAA,UAAT,KAAS,uEAAD,CAAC;AACxB,aAAO,IAAI,CAAC;AAAA,eAAM,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,OAAJ,CAAY,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,KAAX,CAAZ,CAAR,EAAwC,CAAxC,CAAN;AAAA,OAAD,CAAX;AACD;AAZH;;AAAA;AAAA,EAA2B,UAA3B;AACE;;AACgB,KAAA,CAAA,SAAA,GAAY,OAAZ;AAYlB,aAAa,CAAC,aAAd,CAA4B,KAA5B;AAEA;;AAEG;;AACH,WAAa,IAAb;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;AAAA;AAGE;;;;;AAKG;AACH,mBAAM,CAAN,EAAe;AACb,aAAO,IAAI,CAAC;AAAA,eAAM,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,GAAG,CAAC,IAAJ,CAAS,GAAG,CAAC,QAAJ,CAAa,CAAb,CAAT,CAAX,CAAN;AAAA,OAAD,CAAX;AACD;AAXH;;AAAA;AAAA,EAA0B,UAA1B;AACE;;AACgB,IAAA,CAAA,SAAA,GAAY,MAAZ;AAWlB,aAAa,CAAC,aAAd,CAA4B,IAA5B;AAEA,OAAM,SAAU,mBAAV,CAA8B,UAA9B,EAAoD;AACxD,SAAO,UAAU,CAAC,YAAX,EAAP;AACD;AAED,OAAM,SAAU,qBAAV,CACF,MADE,EAE0C;AAAA,MAA5C,aAA4C,uEAAF,EAAE;AAC9C,SAAO,sBAAsB,CACzB,MADyB,EACjB,aAAa,CAAC,gBAAd,CAA+B,MAA/B,GAAwC,YADvB,EAEzB,aAFyB,EAEV,YAFU,CAA7B;AAGD;AAED,OAAM,SAAU,aAAV,CAAwB,UAAxB,EAC2D;AAC/D,MAAI,UAAU,IAAI,IAAlB,EAAwB;AACtB,QAAM,MAAM,GAA6B,EAAzC;AACA,IAAA,MAAM,CAAC,WAAD,CAAN,GAAsB,QAAtB;AACA,IAAA,MAAM,CAAC,QAAD,CAAN,GAAmB,EAAnB;AACA,WAAO,qBAAqB,CAAC,MAAD,CAA5B;AACD;;AACD,MAAI,OAAO,UAAP,KAAsB,QAA1B,EAAoC;AAClC,QAAM,OAAM,GAA6B,EAAzC;AACA,IAAA,OAAM,CAAC,WAAD,CAAN,GAAsB,UAAtB;AACA,IAAA,OAAM,CAAC,QAAD,CAAN,GAAmB,EAAnB;AACA,WAAO,qBAAqB,CAAC,OAAD,CAA5B;AACD,GALD,MAKO,IAAI,UAAU,YAAY,UAA1B,EAAsC;AAC3C,WAAO,UAAP;AACD,GAFM,MAEA;AACL,WAAO,qBAAqB,CAAC,UAAD,CAA5B;AACD;AACF","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport {ActivationIdentifier} from './keras_format/activation_config';\nimport {deserializeKerasObject} from './utils/generic_utils';\n\n/**\n * Base class for Activations.\n *\n * Special note: due to cross-language compatibility reasons, the\n * static readonly className field in this family of classes must be set to\n * the initialLowerCamelCase name of the activation.\n */\nexport abstract class Activation extends serialization.Serializable {\n  abstract apply(tensor: Tensor, axis?: number): Tensor;\n  getConfig(): serialization.ConfigDict {\n    return {};\n  }\n}\n\n/**\n * Exponential linear unit (ELU).\n * Reference: https://arxiv.org/abs/1511.07289\n */\nexport class Elu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'elu';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x: Input.\n   * @param alpha: Scaling factor the negative section.\n   * @return Output of the ELU activation.\n   */\n  apply(x: Tensor, alpha = 1): Tensor {\n    return K.elu(x, alpha);\n  }\n}\nserialization.registerClass(Elu);\n\n/**\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\n * Notes:\n *   - To be used together with the initialization \"lecunNormal\".\n *   - To be used together with the dropout variant \"AlphaDropout\".\n */\nexport class Selu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'selu';\n  apply(x: Tensor): Tensor {\n    return tfc.selu(x);\n  }\n}\nserialization.registerClass(Selu);\n\n/**\n *  Rectified linear unit\n */\nexport class Relu extends Activation {\n  /** @nocollapse */\n  static readonly className = 'relu';\n  apply(x: Tensor): Tensor {\n    return tfc.relu(x);\n  }\n}\nserialization.registerClass(Relu);\n\n/**\n * Rectified linear unit activation maxing out at 6.0.\n */\nexport class Relu6 extends Activation {\n  /** @nocollapse */\n  static readonly className = 'relu6';\n  apply(x: Tensor): Tensor {\n    return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n  }\n}\nserialization.registerClass(Relu6);\n\n//* Linear activation (no-op) */\nexport class Linear extends Activation {\n  /** @nocollapse */\n  static readonly className = 'linear';\n  apply(x: Tensor): Tensor {\n    return x;\n  }\n}\nserialization.registerClass(Linear);\n\n/**\n * Sigmoid activation function.\n */\nexport class Sigmoid extends Activation {\n  /** @nocollapse */\n  static readonly className = 'sigmoid';\n  apply(x: Tensor): Tensor {\n    return tfc.sigmoid(x);\n  }\n}\nserialization.registerClass(Sigmoid);\n\n/**\n * Segment-wise linear approximation of sigmoid.\n */\nexport class HardSigmoid extends Activation {\n  /** @nocollapse */\n  static readonly className = 'hardSigmoid';\n  apply(x: Tensor): Tensor {\n    return K.hardSigmoid(x);\n  }\n}\nserialization.registerClass(HardSigmoid);\n\n/**\n * Softplus activation function.\n */\nexport class Softplus extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softplus';\n  apply(x: Tensor): Tensor {\n    return tfc.softplus(x);\n  }\n}\nserialization.registerClass(Softplus);\n\n/**\n * Softsign activation function.\n */\nexport class Softsign extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softsign';\n  apply(x: Tensor): Tensor {\n    return K.softsign(x);\n  }\n}\nserialization.registerClass(Softsign);\n\n/**\n * Hyperbolic tangent function.\n */\nexport class Tanh extends Activation {\n  /** @nocollapse */\n  static readonly className = 'tanh';\n  apply(x: Tensor): Tensor {\n    return tfc.tanh(x);\n  }\n}\nserialization.registerClass(Tanh);\n\n/**\n * Softmax activation function\n */\nexport class Softmax extends Activation {\n  /** @nocollapse */\n  static readonly className = 'softmax';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x: Tensor, axis: number = (-1)): Tensor {\n    return tfc.softmax(x, axis);\n  }\n}\nserialization.registerClass(Softmax);\n\n/**\n * Log softmax activation function\n */\nexport class LogSoftmax extends Activation {\n  /** @nocollapse */\n  static readonly className = 'logSoftmax';\n  /**\n   * Calculate the activation function of log softmax:\n   * log( exp(x_i) / sum(exp(x)) )\n   *\n   * @param x Tensor.\n   * @param axis Integer, axis along which the softmax normalization is applied.\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\n   * an error.\n   *\n   * @returns a Tensor of the same shape as x\n   *\n   * @throws ValueError: In case `dim(x) < 2`.\n   */\n  apply(x: Tensor, axis: number = (-1)): Tensor {\n    return tfc.logSoftmax(x, axis);\n  }\n}\nserialization.registerClass(LogSoftmax);\n\n/**\n * Swish activation function\n */\nexport class Swish extends Activation {\n  /** @nocollapse */\n  static readonly className = 'swish';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @param alpha Scaling factor for the sigmoid function.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x: Tensor, alpha = 1): Tensor {\n    return tidy(() => tfc.mul(tfc.sigmoid(tfc.mul(x, alpha)), x));\n  }\n}\nserialization.registerClass(Swish);\n\n/**\n * Mish activation function\n */\nexport class Mish extends Activation {\n  /** @nocollapse */\n  static readonly className = 'mish';\n  /**\n   * Calculate the activation function.\n   *\n   * @param x Tensor.\n   * @returns a Tensor of the same shape as x\n   */\n  apply(x: Tensor): Tensor {\n    return tidy(() => tfc.mul(x, tfc.tanh(tfc.softplus(x))));\n  }\n}\nserialization.registerClass(Mish);\n\nexport function serializeActivation(activation: Activation): string {\n  return activation.getClassName();\n}\n\nexport function deserializeActivation(\n    config: serialization.ConfigDict,\n    customObjects: serialization.ConfigDict = {}): Activation {\n  return deserializeKerasObject(\n      config, serialization.SerializationMap.getMap().classNameMap,\n      customObjects, 'activation');\n}\n\nexport function getActivation(identifier: ActivationIdentifier|\n                              serialization.ConfigDict|Activation): Activation {\n  if (identifier == null) {\n    const config: serialization.ConfigDict = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n  if (typeof identifier === 'string') {\n    const config: serialization.ConfigDict = {};\n    config['className'] = identifier;\n    config['config'] = {};\n    return deserializeActivation(config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}