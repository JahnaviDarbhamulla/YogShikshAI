{"ast":null,"code":"import _classCallCheck from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _get from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/get\";\nimport _getPrototypeOf from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/inherits\";\nimport _createSuper from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/createSuper\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { add, greaterEqual, mul, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport var GaussianNoise = /*#__PURE__*/function (_Layer) {\n  _inherits(GaussianNoise, _Layer);\n\n  var _super = _createSuper(GaussianNoise);\n\n  function GaussianNoise(args) {\n    var _this;\n\n    _classCallCheck(this, GaussianNoise);\n\n    _this = _super.call(this, args);\n    _this.supportsMasking = true;\n    _this.stddev = args.stddev;\n    return _this;\n  }\n\n  _createClass(GaussianNoise, [{\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(GaussianNoise.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        stddev: this.stddev\n      };\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n\n      return tidy(function () {\n        _this2.invokeCallHook(inputs, kwargs);\n\n        var input = getExactlyOneTensor(inputs);\n\n        var noised = function noised() {\n          return add(K.randomNormal(input.shape, 0, _this2.stddev), input);\n        };\n\n        var output = K.inTrainPhase(noised, function () {\n          return input;\n        }, kwargs['training'] || false);\n        return output;\n      });\n    }\n  }]);\n\n  return GaussianNoise;\n}(Layer);\n/** @nocollapse */\n\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport var GaussianDropout = /*#__PURE__*/function (_Layer2) {\n  _inherits(GaussianDropout, _Layer2);\n\n  var _super2 = _createSuper(GaussianDropout);\n\n  function GaussianDropout(args) {\n    var _this3;\n\n    _classCallCheck(this, GaussianDropout);\n\n    _this3 = _super2.call(this, args);\n    _this3.supportsMasking = true;\n    _this3.rate = args.rate;\n    return _this3;\n  }\n\n  _createClass(GaussianDropout, [{\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(GaussianDropout.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        rate: this.rate\n      };\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n\n      return tidy(function () {\n        _this4.invokeCallHook(inputs, kwargs);\n\n        var input = getExactlyOneTensor(inputs);\n\n        if (_this4.rate > 0 && _this4.rate < 1) {\n          var noised = function noised() {\n            var stddev = Math.sqrt(_this4.rate / (1 - _this4.rate));\n            return mul(input, K.randomNormal(input.shape, 1, stddev));\n          };\n\n          return K.inTrainPhase(noised, function () {\n            return input;\n          }, kwargs['training'] || false);\n        }\n\n        return input;\n      });\n    }\n  }]);\n\n  return GaussianDropout;\n}(Layer);\n/** @nocollapse */\n\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\n\nexport var AlphaDropout = /*#__PURE__*/function (_Layer3) {\n  _inherits(AlphaDropout, _Layer3);\n\n  var _super3 = _createSuper(AlphaDropout);\n\n  function AlphaDropout(args) {\n    var _this5;\n\n    _classCallCheck(this, AlphaDropout);\n\n    _this5 = _super3.call(this, args);\n    _this5.supportsMasking = true;\n    _this5.rate = args.rate;\n    _this5.noiseShape = args.noiseShape;\n    return _this5;\n  }\n\n  _createClass(AlphaDropout, [{\n    key: \"_getNoiseShape\",\n    value: function _getNoiseShape(inputs) {\n      return this.noiseShape || getExactlyOneTensor(inputs).shape;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(AlphaDropout.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        rate: this.rate\n      };\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this6 = this;\n\n      return tidy(function () {\n        if (_this6.rate < 1 && _this6.rate > 0) {\n          var noiseShape = _this6._getNoiseShape(inputs);\n\n          var droppedInputs = function droppedInputs() {\n            var input = getExactlyOneTensor(inputs);\n            var alpha = 1.6732632423543772848170429916717;\n            var scale = 1.0507009873554804934193349852946;\n            var alphaP = -alpha * scale;\n            var keptIdx = greaterEqual(randomUniform(noiseShape), _this6.rate);\n            keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n            // Get affine transformation params.\n\n            var a = Math.pow((1 - _this6.rate) * (1 + _this6.rate * Math.pow(alphaP, 2)), -0.5);\n            var b = -a * alphaP * _this6.rate; // Apply mask.\n\n            var x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\n            return add(mul(x, a), b);\n          };\n\n          return K.inTrainPhase(droppedInputs, function () {\n            return getExactlyOneTensor(inputs);\n          }, kwargs['training'] || false);\n        }\n\n        return inputs;\n      });\n    }\n  }]);\n\n  return AlphaDropout;\n}(Layer);\n/** @nocollapse */\n\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);","map":{"version":3,"sources":["../../../../../../tfjs-layers/src/layers/noise.ts"],"names":[],"mappings":";;;;;;;AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,SAAQ,GAAR,EAAa,YAAb,EAA2B,GAA3B,EAAgC,aAAhC,EAA+C,aAA/C,EAAsE,IAAtE,QAAiF,uBAAjF;AAEA,OAAO,KAAK,CAAZ,MAAmB,yBAAnB;AACA,SAAQ,KAAR,QAA+B,oBAA/B;AAGA,SAAQ,mBAAR,QAAkC,sBAAlC;AAOA,WAAa,aAAb;AAAA;;AAAA;;AAKE,yBAAY,IAAZ,EAAmC;AAAA;;AAAA;;AACjC,8BAAM,IAAN;AACA,UAAK,eAAL,GAAuB,IAAvB;AACA,UAAK,MAAL,GAAc,IAAI,CAAC,MAAnB;AAHiC;AAIlC;;AATH;AAAA;AAAA,WAWE,4BAAmB,UAAnB,EAA4C;AAC1C,aAAO,UAAP;AACD;AAbH;AAAA;AAAA,WAeE,qBAAS;AACP,UAAM,UAAU,+EAAhB;;AACA,UAAM,MAAM,GAAG;AAAC,QAAA,MAAM,EAAE,KAAK;AAAd,OAAf;AACA,MAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,aAAO,MAAP;AACD;AApBH;AAAA;AAAA,WAsBE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;AAAA;;AAC1C,aAAO,IAAI,CAAC,YAAK;AACf,QAAA,MAAI,CAAC,cAAL,CAAoB,MAApB,EAA4B,MAA5B;;AACA,YAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;;AACA,YAAM,MAAM,GAAG,SAAT,MAAS;AAAA,iBACX,GAAG,CAAC,CAAC,CAAC,YAAF,CAAe,KAAK,CAAC,KAArB,EAA4B,CAA5B,EAA+B,MAAI,CAAC,MAApC,CAAD,EAA8C,KAA9C,CADQ;AAAA,SAAf;;AAEA,YAAM,MAAM,GACR,CAAC,CAAC,YAAF,CAAe,MAAf,EAAuB;AAAA,iBAAM,KAAN;AAAA,SAAvB,EAAoC,MAAM,CAAC,UAAD,CAAN,IAAsB,KAA1D,CADJ;AAEA,eAAO,MAAP;AACD,OARU,CAAX;AASD;AAhCH;;AAAA;AAAA,EAAmC,KAAnC;AACE;;AACO,aAAA,CAAA,SAAA,GAAY,eAAZ;AAgCT,aAAa,CAAC,aAAd,CAA4B,aAA5B;AAOA,WAAa,eAAb;AAAA;;AAAA;;AAKE,2BAAY,IAAZ,EAAqC;AAAA;;AAAA;;AACnC,gCAAM,IAAN;AACA,WAAK,eAAL,GAAuB,IAAvB;AACA,WAAK,IAAL,GAAY,IAAI,CAAC,IAAjB;AAHmC;AAIpC;;AATH;AAAA;AAAA,WAWE,4BAAmB,UAAnB,EAA4C;AAC1C,aAAO,UAAP;AACD;AAbH;AAAA;AAAA,WAeE,qBAAS;AACP,UAAM,UAAU,iFAAhB;;AACA,UAAM,MAAM,GAAG;AAAC,QAAA,IAAI,EAAE,KAAK;AAAZ,OAAf;AACA,MAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,aAAO,MAAP;AACD;AApBH;AAAA;AAAA,WAsBE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;AAAA;;AAC1C,aAAO,IAAI,CAAC,YAAK;AACf,QAAA,MAAI,CAAC,cAAL,CAAoB,MAApB,EAA4B,MAA5B;;AACA,YAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;;AACA,YAAI,MAAI,CAAC,IAAL,GAAY,CAAZ,IAAiB,MAAI,CAAC,IAAL,GAAY,CAAjC,EAAoC;AAClC,cAAM,MAAM,GAAG,SAAT,MAAS,GAAK;AAClB,gBAAM,MAAM,GAAG,IAAI,CAAC,IAAL,CAAU,MAAI,CAAC,IAAL,IAAa,IAAI,MAAI,CAAC,IAAtB,CAAV,CAAf;AACA,mBAAO,GAAG,CAAC,KAAD,EAAQ,CAAC,CAAC,YAAF,CAAe,KAAK,CAAC,KAArB,EAA4B,CAA5B,EAA+B,MAA/B,CAAR,CAAV;AACD,WAHD;;AAIA,iBAAO,CAAC,CAAC,YAAF,CAAe,MAAf,EAAuB;AAAA,mBAAM,KAAN;AAAA,WAAvB,EAAoC,MAAM,CAAC,UAAD,CAAN,IAAsB,KAA1D,CAAP;AACD;;AACD,eAAO,KAAP;AACD,OAXU,CAAX;AAYD;AAnCH;;AAAA;AAAA,EAAqC,KAArC;AACE;;AACO,eAAA,CAAA,SAAA,GAAY,iBAAZ;AAmCT,aAAa,CAAC,aAAd,CAA4B,eAA5B;AAYA;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA4BG;;AACH,WAAa,YAAb;AAAA;;AAAA;;AAME,wBAAY,IAAZ,EAAkC;AAAA;;AAAA;;AAChC,gCAAM,IAAN;AACA,WAAK,eAAL,GAAuB,IAAvB;AACA,WAAK,IAAL,GAAY,IAAI,CAAC,IAAjB;AACA,WAAK,UAAL,GAAkB,IAAI,CAAC,UAAvB;AAJgC;AAKjC;;AAXH;AAAA;AAAA,WAaE,wBAAe,MAAf,EAAsC;AACpC,aAAO,KAAK,UAAL,IAAmB,mBAAmB,CAAC,MAAD,CAAnB,CAA4B,KAAtD;AACD;AAfH;AAAA;AAAA,WAiBE,4BAAmB,UAAnB,EAA4C;AAC1C,aAAO,UAAP;AACD;AAnBH;AAAA;AAAA,WAqBE,qBAAS;AACP,UAAM,UAAU,8EAAhB;;AACA,UAAM,MAAM,GAAG;AAAC,QAAA,IAAI,EAAE,KAAK;AAAZ,OAAf;AACA,MAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,aAAO,MAAP;AACD;AA1BH;AAAA;AAAA,WA4BE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;AAAA;;AAC1C,aAAO,IAAI,CAAC,YAAK;AACf,YAAI,MAAI,CAAC,IAAL,GAAY,CAAZ,IAAiB,MAAI,CAAC,IAAL,GAAY,CAAjC,EAAoC;AAClC,cAAM,UAAU,GAAG,MAAI,CAAC,cAAL,CAAoB,MAApB,CAAnB;;AAEA,cAAM,aAAa,GAAG,SAAhB,aAAgB,GAAK;AACzB,gBAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;AAEA,gBAAM,KAAK,GAAG,iCAAd;AACA,gBAAM,KAAK,GAAG,iCAAd;AAEA,gBAAM,MAAM,GAAG,CAAC,KAAD,GAAS,KAAxB;AAEA,gBAAI,OAAO,GAAG,YAAY,CAAC,aAAa,CAAC,UAAD,CAAd,EAA4B,MAAI,CAAC,IAAjC,CAA1B;AAEA,YAAA,OAAO,GAAG,CAAC,CAAC,IAAF,CAAO,OAAP,EAAgB,SAAhB,CAAV,CAVyB,CAUc;AAEvC;;AACA,gBAAM,CAAC,YAAI,CAAC,IAAI,MAAI,CAAC,IAAV,KAAmB,IAAI,MAAI,CAAC,IAAL,YAAY,MAAZ,EAAsB,CAAtB,CAAvB,CAAJ,EAAwD,CAAC,GAAzD,CAAP;AACA,gBAAM,CAAC,GAAG,CAAC,CAAD,GAAK,MAAL,GAAc,MAAI,CAAC,IAA7B,CAdyB,CAgBzB;;AACA,gBAAM,CAAC,GAAG,GAAG,CAAC,GAAG,CAAC,KAAD,EAAQ,OAAR,CAAJ,EAAsB,GAAG,CAAC,GAAG,CAAC,OAAD,EAAU,CAAC,CAAX,CAAJ,EAAmB,MAAnB,CAAzB,CAAb;AAEA,mBAAO,GAAG,CAAC,GAAG,CAAC,CAAD,EAAI,CAAJ,CAAJ,EAAY,CAAZ,CAAV;AACD,WApBD;;AAqBA,iBAAO,CAAC,CAAC,YAAF,CACH,aADG,EACY;AAAA,mBAAM,mBAAmB,CAAC,MAAD,CAAzB;AAAA,WADZ,EAEH,MAAM,CAAC,UAAD,CAAN,IAAsB,KAFnB,CAAP;AAGD;;AACD,eAAO,MAAP;AACD,OA9BU,CAAX;AA+BD;AA5DH;;AAAA;AAAA,EAAkC,KAAlC;AACE;;AACO,YAAA,CAAA,SAAA,GAAY,cAAZ;AA4DT,aAAa,CAAC,aAAd,CAA4B,YAA5B","sourcesContent":["/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\r\n\r\n/**\r\n * TensorFlow.js Layers: Noise Layers.\r\n */\r\n\r\nimport {add, greaterEqual, mul, randomUniform, serialization, Tensor, tidy} from '@tensorflow/tfjs-core';\r\n\r\nimport * as K from '../backend/tfjs_backend';\r\nimport {Layer, LayerArgs} from '../engine/topology';\r\nimport {Shape} from '../keras_format/common';\r\nimport {Kwargs} from '../types';\r\nimport {getExactlyOneTensor} from '../utils/types_utils';\r\n\r\nexport declare interface GaussianNoiseArgs extends LayerArgs {\r\n  /** Standard Deviation.  */\r\n  stddev: number;\r\n}\r\n\r\nexport class GaussianNoise extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'GaussianNoise';\r\n  readonly stddev: number;\r\n\r\n  constructor(args: GaussianNoiseArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.stddev = args.stddev;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {stddev: this.stddev};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      this.invokeCallHook(inputs, kwargs);\r\n      const input = getExactlyOneTensor(inputs);\r\n      const noised = () =>\r\n          add(K.randomNormal(input.shape, 0, this.stddev), input);\r\n      const output =\r\n          K.inTrainPhase(noised, () => input, kwargs['training'] || false);\r\n      return output;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(GaussianNoise);\r\n\r\nexport declare interface GaussianDropoutArgs extends LayerArgs {\r\n  /** drop probability.  */\r\n  rate: number;\r\n}\r\n\r\nexport class GaussianDropout extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'GaussianDropout';\r\n  readonly rate: number;\r\n\r\n  constructor(args: GaussianDropoutArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.rate = args.rate;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {rate: this.rate};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      this.invokeCallHook(inputs, kwargs);\r\n      const input = getExactlyOneTensor(inputs);\r\n      if (this.rate > 0 && this.rate < 1) {\r\n        const noised = () => {\r\n          const stddev = Math.sqrt(this.rate / (1 - this.rate));\r\n          return mul(input, K.randomNormal(input.shape, 1, stddev));\r\n        };\r\n        return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\r\n      }\r\n      return input;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(GaussianDropout);\r\n\r\nexport declare interface AlphaDropoutArgs extends LayerArgs {\r\n  /** drop probability.  */\r\n  rate: number;\r\n  /**\r\n   * A 1-D `Tensor` of type `int32`, representing the\r\n   * shape for randomly generated keep/drop flags.\r\n   */\r\n  noiseShape?: Shape;\r\n}\r\n\r\n/**\r\n * Applies Alpha Dropout to the input.\r\n *\r\n * As it is a regularization layer, it is only active at training time.\r\n *\r\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\r\n * to their original values, in order to ensure the self-normalizing property\r\n * even after this dropout.\r\n * Alpha Dropout fits well to Scaled Exponential Linear Units\r\n * by randomly setting activations to the negative saturation value.\r\n *\r\n * Arguments:\r\n *   - `rate`: float, drop probability (as with `Dropout`).\r\n *     The multiplicative noise will have\r\n *     standard deviation `sqrt(rate / (1 - rate))`.\r\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\r\n *     shape for randomly generated keep/drop flags.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the keyword argument `inputShape`\r\n *   (tuple of integers, does not include the samples axis)\r\n *   when using this layer as the first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as input.\r\n *\r\n * References:\r\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\r\n */\r\nexport class AlphaDropout extends Layer {\r\n  /** @nocollapse */\r\n  static className = 'AlphaDropout';\r\n  readonly rate: number;\r\n  readonly noiseShape: Shape;\r\n\r\n  constructor(args: AlphaDropoutArgs) {\r\n    super(args);\r\n    this.supportsMasking = true;\r\n    this.rate = args.rate;\r\n    this.noiseShape = args.noiseShape;\r\n  }\r\n\r\n  _getNoiseShape(inputs: Tensor|Tensor[]) {\r\n    return this.noiseShape || getExactlyOneTensor(inputs).shape;\r\n  }\r\n\r\n  computeOutputShape(inputShape: Shape|Shape[]): Shape|Shape[] {\r\n    return inputShape;\r\n  }\r\n\r\n  getConfig() {\r\n    const baseConfig = super.getConfig();\r\n    const config = {rate: this.rate};\r\n    Object.assign(config, baseConfig);\r\n    return config;\r\n  }\r\n\r\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\r\n    return tidy(() => {\r\n      if (this.rate < 1 && this.rate > 0) {\r\n        const noiseShape = this._getNoiseShape(inputs);\r\n\r\n        const droppedInputs = () => {\r\n          const input = getExactlyOneTensor(inputs);\r\n\r\n          const alpha = 1.6732632423543772848170429916717;\r\n          const scale = 1.0507009873554804934193349852946;\r\n\r\n          const alphaP = -alpha * scale;\r\n\r\n          let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\r\n\r\n          keptIdx = K.cast(keptIdx, 'float32');  // get default dtype.\r\n\r\n          // Get affine transformation params.\r\n          const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\r\n          const b = -a * alphaP * this.rate;\r\n\r\n          // Apply mask.\r\n          const x = add(mul(input, keptIdx), mul(add(keptIdx, -1), alphaP));\r\n\r\n          return add(mul(x, a), b);\r\n        };\r\n        return K.inTrainPhase(\r\n            droppedInputs, () => getExactlyOneTensor(inputs),\r\n            kwargs['training'] || false);\r\n      }\r\n      return inputs;\r\n    });\r\n  }\r\n}\r\nserialization.registerClass(AlphaDropout);\r\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}