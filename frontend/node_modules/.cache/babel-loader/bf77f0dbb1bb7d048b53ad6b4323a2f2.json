{"ast":null,"code":"import _slicedToArray from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\nimport _defineProperty from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/defineProperty\";\nimport _classCallCheck from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _get from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/get\";\nimport _getPrototypeOf from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/getPrototypeOf\";\nimport _inherits from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/inherits\";\nimport _createSuper from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/createSuper\";\nimport _createForOfIteratorHelper from \"C:/Users/RajaJ/Downloads/YogaIntelliJ-main/YogaIntelliJ-main/frontend/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, reshape, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\n\nexport function batchNormalization(x, mean, variance, beta, gamma) {\n  var epsilon = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1e-3;\n  var out;\n\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n  } else {\n    throw new NotImplementedError(\"batchNormalization is not implemented for array of rank \".concat(x.rank, \" \") + \"yet\");\n  }\n\n  return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\n\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var targetShape = [];\n\n    var _iterator = _createForOfIteratorHelper(math_utils.range(0, x.rank)),\n        _step;\n\n    try {\n      for (_iterator.s(); !(_step = _iterator.n()).done;) {\n        var axis = _step.value;\n\n        if (reductionAxes.indexOf(axis) !== -1) {\n          targetShape.push(1);\n        } else {\n          targetShape.push(x.shape[axis]);\n        }\n      }\n    } catch (err) {\n      _iterator.e(err);\n    } finally {\n      _iterator.f();\n    }\n\n    var broadcastMean = reshape(mean, targetShape);\n    var broadcastVariance = reshape(variance, targetShape);\n    var broadcastGamma = gamma == null ? null : reshape(gamma, targetShape);\n    var broadcastBeta = beta == null ? null : reshape(beta, targetShape);\n    var normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\n\n\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n\n  if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\nexport var BatchNormalization = /*#__PURE__*/function (_Layer) {\n  _inherits(BatchNormalization, _Layer);\n\n  var _super = _createSuper(BatchNormalization);\n\n  function BatchNormalization(args) {\n    var _this;\n\n    _classCallCheck(this, BatchNormalization);\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this = _super.call(this, args);\n    _this.supportsMasking = true;\n    _this.axis = args.axis == null ? -1 : args.axis;\n    _this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    _this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    _this.center = args.center == null ? true : args.center;\n    _this.scale = args.scale == null ? true : args.scale;\n    _this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    _this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    _this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || 'zeros');\n    _this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || 'ones');\n    _this.betaConstraint = getConstraint(args.betaConstraint);\n    _this.gammaConstraint = getConstraint(args.gammaConstraint);\n    _this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    _this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    return _this;\n  }\n\n  _createClass(BatchNormalization, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n      var dim = inputShape[axis];\n\n      if (dim == null) {\n        throw new ValueError(\"Axis \".concat(axis, \" of input tensor should have a defined dimension but \") + \"the layer received an input with shape \" + \"\".concat(JSON.stringify(inputShape), \".\"));\n      }\n\n      this.inputSpec = [new InputSpec({\n        ndim: inputShape.length,\n        axes: _defineProperty({}, axis, dim)\n      })];\n      var shape = [dim];\n\n      if (this.scale) {\n        this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n      }\n\n      if (this.center) {\n        this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n      }\n\n      this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n      this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n\n      return tidy(function () {\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        var input = getExactlyOneTensor(inputs);\n        var inputShape = input.shape;\n        var ndim = inputShape.length;\n        var reductionAxes = math_utils.range(0, ndim);\n        var axis = _this2.axis >= 0 ? _this2.axis : _this2.axis + ndim;\n        reductionAxes.splice(axis, 1);\n        var broadcastShape = generic_utils.pyListRepeat(1, ndim);\n        broadcastShape[axis] = inputShape[axis];\n        var sortedReductionAxes = reductionAxes.slice();\n        sortedReductionAxes.sort();\n        var needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n        var normalizeInference = function normalizeInference() {\n          if (needsBroadcasting) {\n            var broadcastMovingMean = reshape(_this2.movingMean.read(), broadcastShape);\n            var broadcastMovingVariance = reshape(_this2.movingVariance.read(), broadcastShape);\n            var broadcastBeta = _this2.center ? reshape(_this2.beta.read(), broadcastShape) : null;\n            var broadcastGamma = _this2.scale ? reshape(_this2.gamma.read(), broadcastShape) : null;\n            return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, _this2.epsilon);\n          } else {\n            return batchNormalization(input, _this2.movingMean.read(), _this2.movingVariance.read(), _this2.beta == null ? null : _this2.beta.read(), _this2.gamma == null ? null : _this2.gamma.read(), _this2.epsilon);\n          }\n        };\n\n        if (!training) {\n          return normalizeInference();\n        }\n\n        var _normalizeBatchInTrai = normalizeBatchInTraining(input, _this2.gamma.read(), _this2.beta.read(), reductionAxes, _this2.epsilon),\n            _normalizeBatchInTrai2 = _slicedToArray(_normalizeBatchInTrai, 3),\n            normedTraining = _normalizeBatchInTrai2[0],\n            mean = _normalizeBatchInTrai2[1],\n            variance = _normalizeBatchInTrai2[2];\n\n        var doMovingAverage = function doMovingAverage(variable, value, momentum) {\n          tfc.tidy(function () {\n            var decay = 1 - momentum;\n            var origValue = variable.read();\n            var updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n            variable.write(tfc.sub(origValue, updateDelta));\n          });\n        }; // Perform updates to moving mean and moving variance for training.\n        // Porting Note: In PyKeras, these updates to `movingMean` and\n        //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n        //   `update`s using the `add_update()` method. Here we do it imperatively\n        //   and encapsulate the updates in a function that is invoked\n        //   immediately.\n\n\n        var updateMovingMeanAndVariance = function updateMovingMeanAndVariance() {\n          doMovingAverage(_this2.movingMean, mean, _this2.momentum);\n          doMovingAverage(_this2.movingVariance, variance, _this2.momentum);\n        };\n\n        updateMovingMeanAndVariance();\n        return normedTraining;\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis,\n        momentum: this.momentum,\n        epsilon: this.epsilon,\n        center: this.center,\n        scale: this.scale,\n        betaInitializer: serializeInitializer(this.betaInitializer),\n        gammaInitializer: serializeInitializer(this.gammaInitializer),\n        movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n        movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n        betaRegularizer: serializeRegularizer(this.betaRegularizer),\n        gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n        betaConstraint: serializeConstraint(this.betaConstraint),\n        gammaConstraint: serializeConstraint(this.gammaConstraint)\n      };\n\n      var baseConfig = _get(_getPrototypeOf(BatchNormalization.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return BatchNormalization;\n}(Layer);\n/** @nocollapse */\n\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport var LayerNormalization = /*#__PURE__*/function (_Layer2) {\n  _inherits(LayerNormalization, _Layer2);\n\n  var _super2 = _createSuper(LayerNormalization);\n\n  function LayerNormalization(args) {\n    var _this3;\n\n    _classCallCheck(this, LayerNormalization);\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this3 = _super2.call(this, args);\n    _this3.axis = args.axis == null ? -1 : args.axis;\n\n    if (typeof _this3.axis === 'number') {\n      if (!Number.isInteger(_this3.axis)) {\n        throw new Error(\"Expected axis to be an integer, but received \".concat(_this3.axis));\n      }\n    } else if (Array.isArray(_this3.axis)) {\n      var _iterator2 = _createForOfIteratorHelper(_this3.axis),\n          _step2;\n\n      try {\n        for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n          var axis = _step2.value;\n\n          if (!Number.isInteger(axis)) {\n            throw new Error(\"Expected axis to be an array of integers, \" + \"but received \".concat(JSON.stringify(_this3.axis)));\n          }\n        }\n      } catch (err) {\n        _iterator2.e(err);\n      } finally {\n        _iterator2.f();\n      }\n    } else {\n      throw new Error(\"Expected axis to be an integer or an array of integers, \" + \"but received \".concat(JSON.stringify(_this3.axis)));\n    }\n\n    _this3.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    _this3.center = args.center == null ? true : args.center;\n    _this3.scale = args.scale == null ? true : args.scale;\n    _this3.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    _this3.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    _this3.betaRegularizer = getRegularizer(args.betaRegularizer);\n    _this3.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    _this3.supportsMasking = true;\n    return _this3;\n  }\n\n  _createClass(LayerNormalization, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var nDims = inputShape.length; // Convert axis to array and resolve negatives.\n\n      if (typeof this.axis === 'number') {\n        this.axis = [this.axis];\n      }\n\n      for (var i = 0; i < this.axis.length; ++i) {\n        if (this.axis[i] < 0) {\n          this.axis[i] += nDims;\n        }\n      } // Further validate axes.\n\n\n      var _iterator3 = _createForOfIteratorHelper(this.axis),\n          _step3;\n\n      try {\n        for (_iterator3.s(); !(_step3 = _iterator3.n()).done;) {\n          var axis = _step3.value;\n\n          if (axis < 0 || axis >= nDims) {\n            throw new Error(\"Invalid axis: \".concat(axis));\n          }\n        }\n      } catch (err) {\n        _iterator3.e(err);\n      } finally {\n        _iterator3.f();\n      }\n\n      if (this.axis.length !== generic_utils.unique(this.axis).length) {\n        throw new Error(\"Found duplicate axes in: \".concat(this.axis));\n      }\n\n      var paramShape = this.axis.map(function (axis) {\n        return inputShape[axis];\n      });\n      var trainable = true;\n\n      if (this.scale) {\n        this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n      } else {\n        this.gamma = null;\n      }\n\n      if (this.center) {\n        this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n      } else {\n        this.beta = null;\n      }\n\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n\n      var input = getExactlyOneTensor(inputs);\n      var inputShape = input.shape;\n      var nDims = inputShape.length;\n      return tidy(function () {\n        var keepDims = true;\n\n        var _moments = moments(input, _this4.axis, keepDims),\n            mean = _moments.mean,\n            variance = _moments.variance;\n\n        var broadcastShape = generic_utils.pyListRepeat(1, nDims);\n\n        var _iterator4 = _createForOfIteratorHelper(_this4.axis),\n            _step4;\n\n        try {\n          for (_iterator4.s(); !(_step4 = _iterator4.n()).done;) {\n            var dim = _step4.value;\n            broadcastShape[dim] = inputShape[dim];\n          }\n        } catch (err) {\n          _iterator4.e(err);\n        } finally {\n          _iterator4.f();\n        }\n\n        var broadcast = function broadcast(v) {\n          if (v != null && v.shape.length !== nDims) {\n            return tfc.reshape(v, broadcastShape);\n          } else {\n            return v;\n          }\n        };\n\n        var scale = broadcast(_this4.gamma.read());\n        var offset = broadcast(_this4.beta.read()); // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n        // is a workaround for the limitation of core's batchNormalization?d don't\n        // support broadcasting in their gradients. In addition, the tiling is\n        // necessary to ensure correctness on the browser CPU backend regardless\n        // of forward or backward computation. Remove this workaround once the\n        // limitation is addressed. See .\n\n        var momentsTiling = [];\n        var scaleOffsetTiling = [];\n\n        for (var i = 0; i < nDims; ++i) {\n          if (_this4.axis.indexOf(i) !== -1) {\n            momentsTiling.push(inputShape[i]);\n            scaleOffsetTiling.push(1);\n          } else {\n            momentsTiling.push(1);\n            scaleOffsetTiling.push(inputShape[i]);\n          }\n        }\n\n        mean = tfc.tile(mean, momentsTiling);\n        variance = tfc.tile(variance, momentsTiling);\n        scale = tfc.tile(scale, scaleOffsetTiling);\n        offset = tfc.tile(offset, scaleOffsetTiling);\n        return batchNormalization(input, mean, variance, offset, scale, _this4.epsilon);\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis,\n        epsilon: this.epsilon,\n        center: this.center,\n        scale: this.scale,\n        betaInitializer: serializeInitializer(this.betaInitializer),\n        gammaInitializer: serializeInitializer(this.gammaInitializer),\n        betaRegularizer: serializeRegularizer(this.betaRegularizer),\n        gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n      };\n\n      var baseConfig = _get(_getPrototypeOf(LayerNormalization.prototype), \"getConfig\", this).call(this);\n\n      Object.assign(config, baseConfig);\n      return config;\n    }\n  }]);\n\n  return LayerNormalization;\n}(Layer);\n/** @nocollapse */\n\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);","map":{"version":3,"sources":["../../../../../../tfjs-layers/src/layers/normalization.ts"],"names":[],"mappings":";;;;;;;;;;AAAA;;;;;;;;AAQG;;AAEH;;AAEG;AAEH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,OAAR,EAAiB,OAAjB,EAA0B,aAA1B,EAAyF,IAAzF,EAA+F,IAA/F,QAA0G,uBAA1G;AAEA,SAA0C,aAA1C,EAAyD,mBAAzD,QAAmF,gBAAnF;AACA,SAAQ,SAAR,EAAmB,KAAnB,QAA0C,oBAA1C;AACA,SAAQ,mBAAR,EAA6B,UAA7B,QAA8C,WAA9C;AACA,SAAQ,cAAR,EAA4D,oBAA5D,QAAuF,iBAAvF;AAEA,SAAQ,cAAR,EAA4D,oBAA5D,QAAuF,iBAAvF;AAEA,OAAO,KAAK,aAAZ,MAA+B,wBAA/B;AACA,OAAO,KAAK,UAAZ,MAA4B,qBAA5B;AACA,SAAQ,kBAAR,EAA4B,mBAA5B,QAAsD,sBAAtD;AAGA;;;;;;;;;;;;;AAaG;;AACH,OAAM,SAAU,kBAAV,CACF,CADE,EACS,IADT,EACuB,QADvB,EACyC,IADzC,EACwD,KADxD,EAEY;AAAA,MAAd,OAAc,uEAAJ,IAAI;AAChB,MAAI,GAAJ;;AACA,MAAI,CAAC,CAAC,IAAF,KAAW,CAAf,EAAkB;AAChB,IAAA,GAAG,GAAG,GAAG,CAAC,WAAJ,CACF,CADE,EACa,IADb,EAEF,QAFE,EAE+B,IAF/B,EAGF,KAHE,EAG4B,OAH5B,CAAN;AAID,GALD,MAKO,IAAI,CAAC,CAAC,IAAF,KAAW,CAAf,EAAkB;AACvB;AACA,IAAA,GAAG,GAAG,GAAG,CAAC,WAAJ,CACF,CADE,EACa,IADb,EAEF,QAFE,EAE+B,IAF/B,EAGF,KAHE,EAG4B,OAH5B,CAAN;AAID,GANM,MAMA,IAAI,CAAC,CAAC,IAAF,KAAW,CAAf,EAAkB;AACvB,IAAA,GAAG,GAAG,GAAG,CAAC,WAAJ,CACF,CADE,EACa,IADb,EAEF,QAFE,EAE+B,IAF/B,EAGF,KAHE,EAG4B,OAH5B,CAAN;AAID,GALM,MAKA;AACL,UAAM,IAAI,mBAAJ,CACF,kEAA2D,CAAC,CAAC,IAA7D,cADE,CAAN;AAGD;;AACD,SAAO,GAAP;AACD;AAED;;;;;;;;;;;;;;;;AAgBG;;AACH,SAAS,+BAAT,CACI,CADJ,EACe,KADf,EAC8B,IAD9B,EAC4C,aAD5C,EAEkB;AAAA,MAAd,OAAc,uEAAJ,IAAI;AAChB,SAAO,IAAI,CAAC,YAAK;AACR,QAAM,eAAe,GAAG,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,aAAf,CAAxB;AACA,QAAM,IAAI,GAAG,eAAe,CAAC,IAA7B;AACA,QAAM,QAAQ,GAAG,eAAe,CAAC,QAAjC;AACA,QAAM,MAAM,GACR,kBAAkB,CAAC,CAAD,EAAI,IAAJ,EAAU,QAAV,EAAoB,IAApB,EAA0B,KAA1B,EAAiC,OAAjC,CADtB;AAEA,WAAO,CAAC,MAAD,EAAS,IAAT,EAAe,QAAf,CAAP;AACD,GAPG,CAAX;AAQD;AAED;;;;;;;;;;;;;;;;AAgBG;;;AACH,SAAS,iCAAT,CACI,CADJ,EACe,KADf,EAC8B,IAD9B,EAC4C,aAD5C,EAEkB;AAAA,MAAd,OAAc,uEAAJ,IAAI;AAChB,SAAO,IAAI,CAAC,YAAK;AACR,QAAM,eAAe,GAAG,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,aAAf,CAAxB;AACA,QAAM,IAAI,GAAG,eAAe,CAAC,IAA7B;AACA,QAAM,QAAQ,GAAG,eAAe,CAAC,QAAjC;AACA,QAAM,WAAW,GAAa,EAA9B;;AAJQ,+CAKW,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,CAAC,CAAC,IAAtB,CALX;AAAA;;AAAA;AAKR,0DAAgD;AAAA,YAArC,IAAqC;;AAC9C,YAAI,aAAa,CAAC,OAAd,CAAsB,IAAtB,MAAgC,CAAC,CAArC,EAAwC;AACtC,UAAA,WAAW,CAAC,IAAZ,CAAiB,CAAjB;AACD,SAFD,MAEO;AACL,UAAA,WAAW,CAAC,IAAZ,CAAiB,CAAC,CAAC,KAAF,CAAQ,IAAR,CAAjB;AACD;AACF;AAXO;AAAA;AAAA;AAAA;AAAA;;AAYR,QAAM,aAAa,GAAG,OAAO,CAAC,IAAD,EAAO,WAAP,CAA7B;AACA,QAAM,iBAAiB,GAAG,OAAO,CAAC,QAAD,EAAW,WAAX,CAAjC;AACA,QAAM,cAAc,GAChB,KAAK,IAAI,IAAT,GAAgB,IAAhB,GAAuB,OAAO,CAAC,KAAD,EAAQ,WAAR,CADlC;AAEA,QAAM,aAAa,GACf,IAAI,IAAI,IAAR,GAAe,IAAf,GAAsB,OAAO,CAAC,IAAD,EAAO,WAAP,CADjC;AAEA,QAAM,MAAM,GAAG,kBAAkB,CAC7B,CAD6B,EAC1B,aAD0B,EACX,iBADW,EACQ,aADR,EAE7B,cAF6B,EAEb,OAFa,CAAjC;AAGA,WAAO,CAAC,MAAD,EAAS,IAAT,EAAe,QAAf,CAAP;AACD,GAtBG,CAAX;AAuBD;AAED;;;;;;;;;;AAUG;;;AACH,OAAM,SAAU,wBAAV,CACF,CADE,EACS,KADT,EACwB,IADxB,EACsC,aADtC,EAEY;AAAA,MAAd,OAAc,uEAAJ,IAAI;;AAChB,MAAI,IAAI,CAAC,WAAL,CACI,aAAa,CAAC,KAAd,GAAsB,IAAtB,EADJ,EACkC,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,CAAC,CAAC,IAAF,GAAS,CAA7B,CADlC,CAAJ,EACwE;AACtE,WAAO,+BAA+B,CAClC,CADkC,EAC/B,KAD+B,EACxB,IADwB,EAClB,aADkB,EACH,OADG,CAAtC;AAED,GAJD,MAIO;AACL,WAAO,iCAAiC,CACpC,CADoC,EACjC,KADiC,EAC1B,IAD0B,EACpB,aADoB,EACL,OADK,CAAxC;AAED;AACF;AAoFD,WAAa,kBAAb;AAAA;;AAAA;;AAqBE,8BAAY,IAAZ,EAA8C;AAAA;;AAAA;;AAC5C,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,IAAI,GAAG,EAAP;AACD;;AACD,8BAAM,IAAN;AAEA,UAAK,eAAL,GAAuB,IAAvB;AACA,UAAK,IAAL,GAAY,IAAI,CAAC,IAAL,IAAa,IAAb,GAAoB,CAAC,CAArB,GAAyB,IAAI,CAAC,IAA1C;AACA,UAAK,QAAL,GAAgB,IAAI,CAAC,QAAL,IAAiB,IAAjB,GAAwB,IAAxB,GAA+B,IAAI,CAAC,QAApD;AACA,UAAK,OAAL,GAAe,IAAI,CAAC,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8B,IAAI,CAAC,OAAlD;AACA,UAAK,MAAL,GAAc,IAAI,CAAC,MAAL,IAAe,IAAf,GAAsB,IAAtB,GAA6B,IAAI,CAAC,MAAhD;AACA,UAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4B,IAAI,CAAC,KAA9C;AACA,UAAK,eAAL,GAAuB,cAAc,CAAC,IAAI,CAAC,eAAL,IAAwB,OAAzB,CAArC;AACA,UAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAL,IAAyB,MAA1B,CAAtC;AACA,UAAK,qBAAL,GACI,cAAc,CAAC,IAAI,CAAC,qBAAL,IAA8B,OAA/B,CADlB;AAEA,UAAK,yBAAL,GACI,cAAc,CAAC,IAAI,CAAC,yBAAL,IAAkC,MAAnC,CADlB;AAEA,UAAK,cAAL,GAAsB,aAAa,CAAC,IAAI,CAAC,cAAN,CAAnC;AACA,UAAK,eAAL,GAAuB,aAAa,CAAC,IAAI,CAAC,eAAN,CAApC;AACA,UAAK,eAAL,GAAuB,cAAc,CAAC,IAAI,CAAC,eAAN,CAArC;AACA,UAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAN,CAAtC;AArB4C;AAsB7C;;AA3CH;AAAA;AAAA,WA6CS,eAAM,UAAN,EAA+B;AACpC,MAAA,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;AACA,UAAM,IAAI,GAAG,KAAK,IAAL,IAAa,CAAb,GAAiB,KAAK,IAAtB,GAA8B,KAAK,IAAL,GAAY,UAAU,CAAC,MAAlE;AACA,UAAM,GAAG,GAAG,UAAU,CAAC,IAAD,CAAtB;;AACA,UAAI,GAAG,IAAI,IAAX,EAAiB;AACf,cAAM,IAAI,UAAJ,CACF,eAAQ,IAAR,mHAEG,IAAI,CAAC,SAAL,CAAe,UAAf,CAFH,MADE,CAAN;AAID;;AACD,WAAK,SAAL,GACI,CAAC,IAAI,SAAJ,CAAc;AAAC,QAAA,IAAI,EAAE,UAAU,CAAC,MAAlB;AAA0B,QAAA,IAAI,sBAAI,IAAJ,EAAW,GAAX;AAA9B,OAAd,CAAD,CADJ;AAEA,UAAM,KAAK,GAAG,CAAC,GAAD,CAAd;;AACA,UAAI,KAAK,KAAT,EAAgB;AACd,aAAK,KAAL,GAAa,KAAK,SAAL,CACT,OADS,EACA,KADA,EACO,IADP,EACa,KAAK,gBADlB,EACoC,KAAK,gBADzC,EAET,IAFS,EAEH,KAAK,eAFF,CAAb;AAGD;;AACD,UAAI,KAAK,MAAT,EAAiB;AACf,aAAK,IAAL,GAAY,KAAK,SAAL,CACR,MADQ,EACA,KADA,EACO,IADP,EACa,KAAK,eADlB,EACmC,KAAK,eADxC,EACyD,IADzD,EAER,KAAK,cAFG,CAAZ;AAGD;;AACD,WAAK,UAAL,GAAkB,KAAK,SAAL,CACd,aADc,EACC,KADD,EACQ,IADR,EACc,KAAK,qBADnB,EAC0C,IAD1C,EACgD,KADhD,CAAlB;AAEA,WAAK,cAAL,GAAsB,KAAK,SAAL,CAClB,iBADkB,EACC,KADD,EACQ,IADR,EACc,KAAK,yBADnB,EAC8C,IAD9C,EAElB,KAFkB,CAAtB;AAGA,WAAK,KAAL,GAAa,IAAb;AACD;AA1EH;AAAA;AAAA,WA4EE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;AAAA;;AAC1C,aAAO,IAAI,CAAC,YAAK;AACf,YAAM,QAAQ,GAAG,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqC,MAAM,CAAC,UAAD,CAA5D;AACA,YAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;AACA,YAAM,UAAU,GAAG,KAAK,CAAC,KAAzB;AACA,YAAM,IAAI,GAAG,UAAU,CAAC,MAAxB;AACA,YAAM,aAAa,GAAG,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,IAApB,CAAtB;AACA,YAAM,IAAI,GAAG,MAAI,CAAC,IAAL,IAAa,CAAb,GAAiB,MAAI,CAAC,IAAtB,GAA8B,MAAI,CAAC,IAAL,GAAY,IAAvD;AACA,QAAA,aAAa,CAAC,MAAd,CAAqB,IAArB,EAA2B,CAA3B;AACA,YAAM,cAAc,GAAG,aAAa,CAAC,YAAd,CAA2B,CAA3B,EAA8B,IAA9B,CAAvB;AACA,QAAA,cAAc,CAAC,IAAD,CAAd,GAAuB,UAAU,CAAC,IAAD,CAAjC;AAEA,YAAM,mBAAmB,GAAG,aAAa,CAAC,KAAd,EAA5B;AACA,QAAA,mBAAmB,CAAC,IAApB;AACA,YAAM,iBAAiB,GAAG,CAAC,IAAI,CAAC,WAAL,CACvB,mBADuB,EACF,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,IAApB,EAA0B,KAA1B,CAAgC,CAAhC,EAAmC,IAAI,GAAG,CAA1C,CADE,CAA3B;;AAGA,YAAM,kBAAkB,GAAiB,SAAnC,kBAAmC,GAAK;AAC5C,cAAI,iBAAJ,EAAuB;AACrB,gBAAM,mBAAmB,GACrB,OAAO,CAAC,MAAI,CAAC,UAAL,CAAgB,IAAhB,EAAD,EAAyB,cAAzB,CADX;AAEA,gBAAM,uBAAuB,GACzB,OAAO,CAAC,MAAI,CAAC,cAAL,CAAoB,IAApB,EAAD,EAA6B,cAA7B,CADX;AAEA,gBAAM,aAAa,GACf,MAAI,CAAC,MAAL,GAAc,OAAO,CAAC,MAAI,CAAC,IAAL,CAAU,IAAV,EAAD,EAAmB,cAAnB,CAArB,GAA0D,IAD9D;AAEA,gBAAM,cAAc,GAChB,MAAI,CAAC,KAAL,GAAa,OAAO,CAAC,MAAI,CAAC,KAAL,CAAW,IAAX,EAAD,EAAoB,cAApB,CAApB,GAA0D,IAD9D;AAEA,mBAAO,kBAAkB,CACrB,KADqB,EACd,mBADc,EACO,uBADP,EAErB,aAFqB,EAEN,cAFM,EAEU,MAAI,CAAC,OAFf,CAAzB;AAGD,WAZD,MAYO;AACL,mBAAO,kBAAkB,CACrB,KADqB,EACd,MAAI,CAAC,UAAL,CAAgB,IAAhB,EADc,EACU,MAAI,CAAC,cAAL,CAAoB,IAApB,EADV,EAErB,MAAI,CAAC,IAAL,IAAa,IAAb,GAAoB,IAApB,GAA2B,MAAI,CAAC,IAAL,CAAU,IAAV,EAFN,EAGrB,MAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4B,MAAI,CAAC,KAAL,CAAW,IAAX,EAHP,EAG0B,MAAI,CAAC,OAH/B,CAAzB;AAID;AACF,SAnBD;;AAqBA,YAAI,CAAC,QAAL,EAAe;AACb,iBAAO,kBAAkB,EAAzB;AACD;;AAED,oCAAyC,wBAAwB,CAC7D,KAD6D,EACtD,MAAI,CAAC,KAAL,CAAW,IAAX,EADsD,EACnC,MAAI,CAAC,IAAL,CAAU,IAAV,EADmC,EACjB,aADiB,EAE7D,MAAI,CAAC,OAFwD,CAAjE;AAAA;AAAA,YAAO,cAAP;AAAA,YAAuB,IAAvB;AAAA,YAA6B,QAA7B;;AAIA,YAAM,eAAe,GACjB,SADE,eACF,CAAC,QAAD,EAA0B,KAA1B,EAAyC,QAAzC,EAAmE;AACjE,UAAA,GAAG,CAAC,IAAJ,CAAS,YAAK;AACZ,gBAAM,KAAK,GAAG,IAAI,QAAlB;AACA,gBAAM,SAAS,GAAG,QAAQ,CAAC,IAAT,EAAlB;AACA,gBAAM,WAAW,GAAG,GAAG,CAAC,GAAJ,CAAQ,GAAG,CAAC,GAAJ,CAAQ,SAAR,EAAmB,KAAnB,CAAR,EAAmC,KAAnC,CAApB;AACA,YAAA,QAAQ,CAAC,KAAT,CAAe,GAAG,CAAC,GAAJ,CAAQ,SAAR,EAAmB,WAAnB,CAAf;AACD,WALD;AAMD,SARL,CA7Ce,CAuDf;AACA;AACA;AACA;AACA;AACA;;;AACA,YAAM,2BAA2B,GAAG,SAA9B,2BAA8B,GAAK;AACvC,UAAA,eAAe,CAAC,MAAI,CAAC,UAAN,EAAkB,IAAlB,EAAwB,MAAI,CAAC,QAA7B,CAAf;AACA,UAAA,eAAe,CAAC,MAAI,CAAC,cAAN,EAAsB,QAAtB,EAAgC,MAAI,CAAC,QAArC,CAAf;AACD,SAHD;;AAIA,QAAA,2BAA2B;AAE3B,eAAO,cAAP;AACD,OApEU,CAAX;AAqED;AAlJH;AAAA;AAAA,WAoJE,qBAAS;AACP,UAAM,MAAM,GAA6B;AACvC,QAAA,IAAI,EAAE,KAAK,IAD4B;AAEvC,QAAA,QAAQ,EAAE,KAAK,QAFwB;AAGvC,QAAA,OAAO,EAAE,KAAK,OAHyB;AAIvC,QAAA,MAAM,EAAE,KAAK,MAJ0B;AAKvC,QAAA,KAAK,EAAE,KAAK,KAL2B;AAMvC,QAAA,eAAe,EAAE,oBAAoB,CAAC,KAAK,eAAN,CANE;AAOvC,QAAA,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CAPC;AAQvC,QAAA,qBAAqB,EAAE,oBAAoB,CAAC,KAAK,qBAAN,CARJ;AASvC,QAAA,yBAAyB,EACrB,oBAAoB,CAAC,KAAK,yBAAN,CAVe;AAWvC,QAAA,eAAe,EAAE,oBAAoB,CAAC,KAAK,eAAN,CAXE;AAYvC,QAAA,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CAZC;AAavC,QAAA,cAAc,EAAE,mBAAmB,CAAC,KAAK,cAAN,CAbI;AAcvC,QAAA,eAAe,EAAE,mBAAmB,CAAC,KAAK,eAAN;AAdG,OAAzC;;AAgBA,UAAM,UAAU,oFAAhB;;AACA,MAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,aAAO,MAAP;AACD;AAxKH;;AAAA;AAAA,EAAwC,KAAxC;AACE;;AACO,kBAAA,CAAA,SAAA,GAAY,oBAAZ;AAwKT,aAAa,CAAC,aAAd,CAA4B,kBAA5B;AAkDA,WAAa,kBAAb;AAAA;;AAAA;;AAgBE,8BAAY,IAAZ,EAA8C;AAAA;;AAAA;;AAC5C,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,IAAI,GAAG,EAAP;AACD;;AACD,gCAAM,IAAN;AAEA,WAAK,IAAL,GAAY,IAAI,CAAC,IAAL,IAAa,IAAb,GAAoB,CAAC,CAArB,GAAyB,IAAI,CAAC,IAA1C;;AACA,QAAI,OAAO,OAAK,IAAZ,KAAqB,QAAzB,EAAmC;AACjC,UAAI,CAAC,MAAM,CAAC,SAAP,CAAiB,OAAK,IAAtB,CAAL,EAAkC;AAChC,cAAM,IAAI,KAAJ,wDAC8C,OAAK,IADnD,EAAN;AAED;AACF,KALD,MAKO,IAAI,KAAK,CAAC,OAAN,CAAc,OAAK,IAAnB,CAAJ,EAA8B;AAAA,kDAChB,OAAK,IADW;AAAA;;AAAA;AACnC,+DAA8B;AAAA,cAAnB,IAAmB;;AAC5B,cAAI,CAAC,MAAM,CAAC,SAAP,CAAiB,IAAjB,CAAL,EAA6B;AAC3B,kBAAM,IAAI,KAAJ,CACF,sEACgB,IAAI,CAAC,SAAL,CAAe,OAAK,IAApB,CADhB,CADE,CAAN;AAGD;AACF;AAPkC;AAAA;AAAA;AAAA;AAAA;AAQpC,KARM,MAQA;AACL,YAAM,IAAI,KAAJ,CACF,oFACgB,IAAI,CAAC,SAAL,CAAe,OAAK,IAApB,CADhB,CADE,CAAN;AAGD;;AAED,WAAK,OAAL,GAAe,IAAI,CAAC,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8B,IAAI,CAAC,OAAlD;AACA,WAAK,MAAL,GAAc,IAAI,CAAC,MAAL,IAAe,IAAf,GAAsB,IAAtB,GAA6B,IAAI,CAAC,MAAhD;AACA,WAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4B,IAAI,CAAC,KAA9C;AACA,WAAK,eAAL,GAAuB,cAAc,CAAC,IAAI,CAAC,eAAL,IAAwB,OAAzB,CAArC;AACA,WAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAL,IAAyB,MAA1B,CAAtC;AACA,WAAK,eAAL,GAAuB,cAAc,CAAC,IAAI,CAAC,eAAN,CAArC;AACA,WAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAN,CAAtC;AAEA,WAAK,eAAL,GAAuB,IAAvB;AAlC4C;AAmC7C;;AAnDH;AAAA;AAAA,WAqDS,eAAM,UAAN,EAA+B;AACpC,MAAA,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;AACA,UAAM,KAAK,GAAG,UAAU,CAAC,MAAzB,CAFoC,CAIpC;;AACA,UAAI,OAAO,KAAK,IAAZ,KAAqB,QAAzB,EAAmC;AACjC,aAAK,IAAL,GAAY,CAAC,KAAK,IAAN,CAAZ;AACD;;AACD,WAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAAK,IAAL,CAAU,MAA9B,EAAsC,EAAE,CAAxC,EAA2C;AACzC,YAAI,KAAK,IAAL,CAAU,CAAV,IAAe,CAAnB,EAAsB;AACpB,eAAK,IAAL,CAAU,CAAV,KAAgB,KAAhB;AACD;AACF,OAZmC,CAcpC;;;AAdoC,kDAejB,KAAK,IAfY;AAAA;;AAAA;AAepC,+DAA8B;AAAA,cAAnB,IAAmB;;AAC5B,cAAI,IAAI,GAAG,CAAP,IAAY,IAAI,IAAI,KAAxB,EAA+B;AAC7B,kBAAM,IAAI,KAAJ,yBAA2B,IAA3B,EAAN;AACD;AACF;AAnBmC;AAAA;AAAA;AAAA;AAAA;;AAoBpC,UAAI,KAAK,IAAL,CAAU,MAAV,KAAqB,aAAa,CAAC,MAAd,CAAqB,KAAK,IAA1B,EAAgC,MAAzD,EAAiE;AAC/D,cAAM,IAAI,KAAJ,oCAAsC,KAAK,IAA3C,EAAN;AACD;;AAED,UAAM,UAAU,GAAG,KAAK,IAAL,CAAU,GAAV,CAAc,UAAA,IAAI;AAAA,eAAI,UAAU,CAAC,IAAD,CAAd;AAAA,OAAlB,CAAnB;AAEA,UAAM,SAAS,GAAG,IAAlB;;AACA,UAAI,KAAK,KAAT,EAAgB;AACd,aAAK,KAAL,GAAa,KAAK,SAAL,CACT,OADS,EACA,UADA,EACY,SADZ,EACuB,KAAK,gBAD5B,EAET,KAAK,gBAFI,EAEc,SAFd,CAAb;AAGD,OAJD,MAIO;AACL,aAAK,KAAL,GAAa,IAAb;AACD;;AACD,UAAI,KAAK,MAAT,EAAiB;AACf,aAAK,IAAL,GAAY,KAAK,SAAL,CACR,MADQ,EACA,UADA,EACY,SADZ,EACuB,KAAK,eAD5B,EAER,KAAK,eAFG,EAEc,SAFd,CAAZ;AAGD,OAJD,MAIO;AACL,aAAK,IAAL,GAAY,IAAZ;AACD;;AAED,WAAK,KAAL,GAAa,IAAb;AACD;AAhGH;AAAA;AAAA,WAkGE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;AAAA;;AAC1C,UAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;AACA,UAAM,UAAU,GAAG,KAAK,CAAC,KAAzB;AACA,UAAM,KAAK,GAAG,UAAU,CAAC,MAAzB;AAEA,aAAO,IAAI,CAAC,YAAK;AACf,YAAM,QAAQ,GAAG,IAAjB;;AACA,uBAAuB,OAAO,CAAC,KAAD,EAAQ,MAAI,CAAC,IAAb,EAAmB,QAAnB,CAA9B;AAAA,YAAK,IAAL,YAAK,IAAL;AAAA,YAAW,QAAX,YAAW,QAAX;;AACA,YAAM,cAAc,GAAG,aAAa,CAAC,YAAd,CAA2B,CAA3B,EAA8B,KAA9B,CAAvB;;AAHe,oDAIG,MAAI,CAAC,IAJR;AAAA;;AAAA;AAIf,iEAAyC;AAAA,gBAA9B,GAA8B;AACvC,YAAA,cAAc,CAAC,GAAD,CAAd,GAAsB,UAAU,CAAC,GAAD,CAAhC;AACD;AANc;AAAA;AAAA;AAAA;AAAA;;AAQf,YAAM,SAAS,GAAG,SAAZ,SAAY,CAAC,CAAD,EAAc;AAC9B,cAAI,CAAC,IAAI,IAAL,IAAa,CAAC,CAAC,KAAF,CAAQ,MAAR,KAAmB,KAApC,EAA2C;AACzC,mBAAO,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,cAAf,CAAP;AACD,WAFD,MAEO;AACL,mBAAO,CAAP;AACD;AACF,SAND;;AAQA,YAAI,KAAK,GAAG,SAAS,CAAC,MAAI,CAAC,KAAL,CAAW,IAAX,EAAD,CAArB;AACA,YAAI,MAAM,GAAG,SAAS,CAAC,MAAI,CAAC,IAAL,CAAU,IAAV,EAAD,CAAtB,CAjBe,CAmBf;AACA;AACA;AACA;AACA;AACA;;AACA,YAAM,aAAa,GAAa,EAAhC;AACA,YAAM,iBAAiB,GAAa,EAApC;;AACA,aAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAApB,EAA2B,EAAE,CAA7B,EAAgC;AAC9B,cAAK,MAAI,CAAC,IAAL,CAAuB,OAAvB,CAA+B,CAA/B,MAAsC,CAAC,CAA5C,EAA+C;AAC7C,YAAA,aAAa,CAAC,IAAd,CAAmB,UAAU,CAAC,CAAD,CAA7B;AACA,YAAA,iBAAiB,CAAC,IAAlB,CAAuB,CAAvB;AACD,WAHD,MAGO;AACL,YAAA,aAAa,CAAC,IAAd,CAAmB,CAAnB;AACA,YAAA,iBAAiB,CAAC,IAAlB,CAAuB,UAAU,CAAC,CAAD,CAAjC;AACD;AACF;;AACD,QAAA,IAAI,GAAG,GAAG,CAAC,IAAJ,CAAS,IAAT,EAAe,aAAf,CAAP;AACA,QAAA,QAAQ,GAAG,GAAG,CAAC,IAAJ,CAAS,QAAT,EAAmB,aAAnB,CAAX;AACA,QAAA,KAAK,GAAG,GAAG,CAAC,IAAJ,CAAS,KAAT,EAAgB,iBAAhB,CAAR;AACA,QAAA,MAAM,GAAG,GAAG,CAAC,IAAJ,CAAS,MAAT,EAAiB,iBAAjB,CAAT;AAEA,eAAO,kBAAkB,CACrB,KADqB,EACd,IADc,EACR,QADQ,EACE,MADF,EACU,KADV,EACiB,MAAI,CAAC,OADtB,CAAzB;AAED,OA3CU,CAAX;AA4CD;AAnJH;AAAA;AAAA,WAqJE,qBAAS;AACP,UAAM,MAAM,GAA6B;AACvC,QAAA,IAAI,EAAE,KAAK,IAD4B;AAEvC,QAAA,OAAO,EAAE,KAAK,OAFyB;AAGvC,QAAA,MAAM,EAAE,KAAK,MAH0B;AAIvC,QAAA,KAAK,EAAE,KAAK,KAJ2B;AAKvC,QAAA,eAAe,EAAE,oBAAoB,CAAC,KAAK,eAAN,CALE;AAMvC,QAAA,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CANC;AAOvC,QAAA,eAAe,EAAE,oBAAoB,CAAC,KAAK,eAAN,CAPE;AAQvC,QAAA,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN;AARC,OAAzC;;AAUA,UAAM,UAAU,oFAAhB;;AACA,MAAA,MAAM,CAAC,MAAP,CAAc,MAAd,EAAsB,UAAtB;AACA,aAAO,MAAP;AACD;AAnKH;;AAAA;AAAA,EAAwC,KAAxC;AACE;;AACO,kBAAA,CAAA,SAAA,GAAY,oBAAZ;AAmKT,aAAa,CAAC,aAAd,CAA4B,kBAA5B","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n\n/**\n * Normalization layers.\n */\n\nimport * as tfc from '@tensorflow/tfjs-core';\nimport {moments, reshape, serialization, Tensor, Tensor1D, Tensor2D, Tensor3D, Tensor4D, tidy, util} from '@tensorflow/tfjs-core';\n\nimport {Constraint, ConstraintIdentifier, getConstraint, serializeConstraint} from '../constraints';\nimport {InputSpec, Layer, LayerArgs} from '../engine/topology';\nimport {NotImplementedError, ValueError} from '../errors';\nimport {getInitializer, Initializer, InitializerIdentifier, serializeInitializer} from '../initializers';\nimport {Shape} from '../keras_format/common';\nimport {getRegularizer, Regularizer, RegularizerIdentifier, serializeRegularizer} from '../regularizers';\nimport {Kwargs} from '../types';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport {getExactlyOneShape, getExactlyOneTensor} from '../utils/types_utils';\nimport {LayerVariable} from '../variables';\n\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(\n    x: Tensor, mean: Tensor, variance: Tensor, beta?: Tensor, gamma?: Tensor,\n    epsilon = 1e-3): Tensor {\n  let out: Tensor;\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(\n        x as Tensor2D, mean as Tensor2D | Tensor1D,\n        variance as Tensor2D | Tensor1D, beta as Tensor2D | Tensor1D,\n        gamma as Tensor2D | Tensor1D, epsilon);\n  } else if (x.rank === 3) {\n    // TODO(cais): Check rank; give proper error message.\n    out = tfc.batchNorm3d(\n        x as Tensor3D, mean as Tensor3D | Tensor1D,\n        variance as Tensor3D | Tensor1D, beta as Tensor3D | Tensor1D,\n        gamma as Tensor3D | Tensor1D, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(\n        x as Tensor4D, mean as Tensor4D | Tensor1D,\n        variance as Tensor4D | Tensor1D, beta as Tensor4D | Tensor1D,\n        gamma as Tensor4D | Tensor1D, epsilon);\n  } else {\n    throw new NotImplementedError(\n        `batchNormalization is not implemented for array of rank ${x.rank} ` +\n        `yet`);\n  }\n  return out;\n}\n\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const normed =\n               batchNormalization(x, mean, variance, beta, gamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  return tidy(() => {\n           const meanAndVariance = tfc.moments(x, reductionAxes);\n           const mean = meanAndVariance.mean;\n           const variance = meanAndVariance.variance;\n           const targetShape: number[] = [];\n           for (const axis of math_utils.range(0, x.rank)) {\n             if (reductionAxes.indexOf(axis) !== -1) {\n               targetShape.push(1);\n             } else {\n               targetShape.push(x.shape[axis]);\n             }\n           }\n           const broadcastMean = reshape(mean, targetShape);\n           const broadcastVariance = reshape(variance, targetShape);\n           const broadcastGamma =\n               gamma == null ? null : reshape(gamma, targetShape);\n           const broadcastBeta =\n               beta == null ? null : reshape(beta, targetShape);\n           const normed = batchNormalization(\n               x, broadcastMean, broadcastVariance, broadcastBeta,\n               broadcastGamma, epsilon);\n           return [normed, mean, variance];\n         }) as [Tensor, Tensor, Tensor];\n}\n\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(\n    x: Tensor, gamma: Tensor, beta: Tensor, reductionAxes: number[],\n    epsilon = 1e-3): [Tensor, Tensor, Tensor] {\n  if (util.arraysEqual(\n          reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(\n        x, gamma, beta, reductionAxes, epsilon);\n  }\n}\n\nexport declare interface BatchNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The integer axis that should be normalized (typically the features axis).\n   * Defaults to -1.\n   *\n   * For instance, after a `Conv2D` layer with `data_format=\"channels_first\"`,\n   * set `axis=1` in `batchNormalization`.\n   */\n  axis?: number;\n\n  /**\n   * Momentum of the moving average. Defaults to 0.99.\n   */\n  momentum?: number;\n\n  /**\n   * Small float added to the variance to avoid dividing by zero. Defaults to\n   * 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Defaults to `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear (also e.g. `nn.relu`),\n   * this can be disabled since the scaling will be done by the next layer.\n   * Defaults to `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   *  Defaults to 'zeros'.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   *  Defaults to `ones`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving mean.\n   * Defaults to `zeros`\n   */\n  movingMeanInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the moving variance.\n   *  Defaults to 'Ones'.\n   */\n  movingVarianceInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Constraint for the beta weight.\n   */\n  betaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Constraint for gamma weight.\n   */\n  gammaConstraint?: ConstraintIdentifier|Constraint;\n\n  /**\n   * Regularizer for the beta weight.\n   */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /**\n   * Regularizer for the gamma weight.\n   */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class BatchNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'BatchNormalization';\n  private readonly axis: number;\n  private readonly momentum: number;\n  private readonly epsilon: number;\n  private readonly center: boolean;\n  private readonly scale: boolean;\n  private readonly betaInitializer: Initializer;\n  private readonly gammaInitializer: Initializer;\n  private readonly movingMeanInitializer: Initializer;\n  private readonly movingVarianceInitializer: Initializer;\n  private readonly betaConstraint: Constraint;\n  private readonly gammaConstraint: Constraint;\n  private readonly betaRegularizer: Regularizer;\n  private readonly gammaRegularizer: Regularizer;\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n  private movingMean: LayerVariable;\n  private movingVariance: LayerVariable;\n\n  constructor(args?: BatchNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.supportsMasking = true;\n    this.axis = args.axis == null ? -1 : args.axis;\n    this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.movingMeanInitializer =\n        getInitializer(args.movingMeanInitializer || 'zeros');\n    this.movingVarianceInitializer =\n        getInitializer(args.movingVarianceInitializer || 'ones');\n    this.betaConstraint = getConstraint(args.betaConstraint);\n    this.gammaConstraint = getConstraint(args.gammaConstraint);\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n    const dim = inputShape[axis];\n    if (dim == null) {\n      throw new ValueError(\n          `Axis ${axis} of input tensor should have a defined dimension but ` +\n          `the layer received an input with shape ` +\n          `${JSON.stringify(inputShape)}.`);\n    }\n    this.inputSpec =\n        [new InputSpec({ndim: inputShape.length, axes: {[axis]: dim}})];\n    const shape = [dim];\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', shape, null, this.gammaInitializer, this.gammaRegularizer,\n          true, this.gammaConstraint);\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', shape, null, this.betaInitializer, this.betaRegularizer, true,\n          this.betaConstraint);\n    }\n    this.movingMean = this.addWeight(\n        'moving_mean', shape, null, this.movingMeanInitializer, null, false);\n    this.movingVariance = this.addWeight(\n        'moving_variance', shape, null, this.movingVarianceInitializer, null,\n        false);\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    return tidy(() => {\n      const training = kwargs['training'] == null ? false : kwargs['training'];\n      const input = getExactlyOneTensor(inputs);\n      const inputShape = input.shape;\n      const ndim = inputShape.length;\n      const reductionAxes = math_utils.range(0, ndim);\n      const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n      reductionAxes.splice(axis, 1);\n      const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n      broadcastShape[axis] = inputShape[axis];\n\n      const sortedReductionAxes = reductionAxes.slice();\n      sortedReductionAxes.sort();\n      const needsBroadcasting = !util.arraysEqual(\n          sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n      const normalizeInference: () => Tensor = () => {\n        if (needsBroadcasting) {\n          const broadcastMovingMean =\n              reshape(this.movingMean.read(), broadcastShape);\n          const broadcastMovingVariance =\n              reshape(this.movingVariance.read(), broadcastShape);\n          const broadcastBeta =\n              this.center ? reshape(this.beta.read(), broadcastShape) : null;\n          const broadcastGamma =\n              this.scale ? reshape(this.gamma.read(), broadcastShape) : null;\n          return batchNormalization(\n              input, broadcastMovingMean, broadcastMovingVariance,\n              broadcastBeta, broadcastGamma, this.epsilon);\n        } else {\n          return batchNormalization(\n              input, this.movingMean.read(), this.movingVariance.read(),\n              this.beta == null ? null : this.beta.read(),\n              this.gamma == null ? null : this.gamma.read(), this.epsilon);\n        }\n      };\n\n      if (!training) {\n        return normalizeInference();\n      }\n\n      const [normedTraining, mean, variance] = normalizeBatchInTraining(\n          input, this.gamma.read(), this.beta.read(), reductionAxes,\n          this.epsilon);\n\n      const doMovingAverage =\n          (variable: LayerVariable, value: Tensor, momentum: number): void => {\n            tfc.tidy(() => {\n              const decay = 1 - momentum;\n              const origValue = variable.read();\n              const updateDelta = tfc.mul(tfc.sub(origValue, value), decay);\n              variable.write(tfc.sub(origValue, updateDelta));\n            });\n          };\n\n      // Perform updates to moving mean and moving variance for training.\n      // Porting Note: In PyKeras, these updates to `movingMean` and\n      //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n      //   `update`s using the `add_update()` method. Here we do it imperatively\n      //   and encapsulate the updates in a function that is invoked\n      //   immediately.\n      const updateMovingMeanAndVariance = () => {\n        doMovingAverage(this.movingMean, mean, this.momentum);\n        doMovingAverage(this.movingVariance, variance, this.momentum);\n      };\n      updateMovingMeanAndVariance();\n\n      return normedTraining;\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      momentum: this.momentum,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n      movingVarianceInitializer:\n          serializeInitializer(this.movingVarianceInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n      betaConstraint: serializeConstraint(this.betaConstraint),\n      gammaConstraint: serializeConstraint(this.gammaConstraint)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(BatchNormalization);\n\nexport interface LayerNormalizationLayerArgs extends LayerArgs {\n  /**\n   * The axis or axes that should be normalized (typically, the feature axis.)\n   * Defaults to -1 (the last axis.)\n   */\n  axis?: number|number[];\n\n  /**\n   * A small positive float added to variance to avoid divison by zero.\n   * Defaults to 1e-3.\n   */\n  epsilon?: number;\n\n  /**\n   * If `true`, add offset of `beta` to normalized tensor.\n   * If `false`, `beta` is ignored.\n   * Default: `true`.\n   */\n  center?: boolean;\n\n  /**\n   * If `true`, multiply output by `gamma`.\n   * If `false`, `gamma` is not used.\n   * When the next layer is linear, this can be disabled since scaling will\n   * be done by the next layer.\n   * Default: `true`.\n   */\n  scale?: boolean;\n\n  /**\n   * Initializer for the beta weight.\n   * Default: `'zeros'`.\n   */\n  betaInitializer?: InitializerIdentifier|Initializer;\n\n  /**\n   * Initializer for the gamma weight.\n   * Default: `'ones'`.\n   */\n  gammaInitializer?: InitializerIdentifier|Initializer;\n\n  /** Regularizer for the beta weight. */\n  betaRegularizer?: RegularizerIdentifier|Regularizer;\n\n  /** Regularizer for the gamma weight. */\n  gammaRegularizer?: RegularizerIdentifier|Regularizer;\n}\n\nexport class LayerNormalization extends Layer {\n  /** @nocollapse */\n  static className = 'LayerNormalization';\n\n  private axis: number|number[];\n  readonly epsilon: number;\n  readonly center: boolean;\n  readonly scale: boolean;\n  readonly betaInitializer: Initializer;\n  readonly gammaInitializer: Initializer;\n  readonly betaRegularizer: Regularizer;\n  readonly gammaRegularizer: Regularizer;\n\n  private gamma: LayerVariable;\n  private beta: LayerVariable;\n\n  constructor(args?: LayerNormalizationLayerArgs) {\n    if (args == null) {\n      args = {};\n    }\n    super(args);\n\n    this.axis = args.axis == null ? -1 : args.axis;\n    if (typeof this.axis === 'number') {\n      if (!Number.isInteger(this.axis)) {\n        throw new Error(\n            `Expected axis to be an integer, but received ${this.axis}`);\n      }\n    } else if (Array.isArray(this.axis)) {\n      for (const axis of this.axis) {\n        if (!Number.isInteger(axis)) {\n          throw new Error(\n              `Expected axis to be an array of integers, ` +\n              `but received ${JSON.stringify(this.axis)}`);\n        }\n      }\n    } else {\n      throw new Error(\n          `Expected axis to be an integer or an array of integers, ` +\n          `but received ${JSON.stringify(this.axis)}`);\n    }\n\n    this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    this.center = args.center == null ? true : args.center;\n    this.scale = args.scale == null ? true : args.scale;\n    this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n\n    this.supportsMasking = true;\n  }\n\n  public build(inputShape: Shape|Shape[]): void {\n    inputShape = getExactlyOneShape(inputShape);\n    const nDims = inputShape.length;\n\n    // Convert axis to array and resolve negatives.\n    if (typeof this.axis === 'number') {\n      this.axis = [this.axis];\n    }\n    for (let i = 0; i < this.axis.length; ++i) {\n      if (this.axis[i] < 0) {\n        this.axis[i] += nDims;\n      }\n    }\n\n    // Further validate axes.\n    for (const axis of this.axis) {\n      if (axis < 0 || axis >= nDims) {\n        throw new Error(`Invalid axis: ${axis}`);\n      }\n    }\n    if (this.axis.length !== generic_utils.unique(this.axis).length) {\n      throw new Error(`Found duplicate axes in: ${this.axis}`);\n    }\n\n    const paramShape = this.axis.map(axis => inputShape[axis]) as number[];\n\n    const trainable = true;\n    if (this.scale) {\n      this.gamma = this.addWeight(\n          'gamma', paramShape, 'float32', this.gammaInitializer,\n          this.gammaRegularizer, trainable);\n    } else {\n      this.gamma = null;\n    }\n    if (this.center) {\n      this.beta = this.addWeight(\n          'beta', paramShape, 'float32', this.betaInitializer,\n          this.betaRegularizer, trainable);\n    } else {\n      this.beta = null;\n    }\n\n    this.built = true;\n  }\n\n  call(inputs: Tensor|Tensor[], kwargs: Kwargs): Tensor|Tensor[] {\n    const input = getExactlyOneTensor(inputs);\n    const inputShape = input.shape;\n    const nDims = inputShape.length;\n\n    return tidy(() => {\n      const keepDims = true;\n      let {mean, variance} = moments(input, this.axis, keepDims);\n      const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n      for (const dim of this.axis as number[]) {\n        broadcastShape[dim] = inputShape[dim];\n      }\n\n      const broadcast = (v: Tensor) => {\n        if (v != null && v.shape.length !== nDims) {\n          return tfc.reshape(v, broadcastShape);\n        } else {\n          return v;\n        }\n      };\n\n      let scale = broadcast(this.gamma.read());\n      let offset = broadcast(this.beta.read());\n\n      // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n      // is a workaround for the limitation of core's batchNormalization?d don't\n      // support broadcasting in their gradients. In addition, the tiling is\n      // necessary to ensure correctness on the browser CPU backend regardless\n      // of forward or backward computation. Remove this workaround once the\n      // limitation is addressed. See .\n      const momentsTiling: number[] = [];\n      const scaleOffsetTiling: number[] = [];\n      for (let i = 0; i < nDims; ++i) {\n        if ((this.axis as number[]).indexOf(i) !== -1) {\n          momentsTiling.push(inputShape[i]);\n          scaleOffsetTiling.push(1);\n        } else {\n          momentsTiling.push(1);\n          scaleOffsetTiling.push(inputShape[i]);\n        }\n      }\n      mean = tfc.tile(mean, momentsTiling);\n      variance = tfc.tile(variance, momentsTiling);\n      scale = tfc.tile(scale, scaleOffsetTiling);\n      offset = tfc.tile(offset, scaleOffsetTiling);\n\n      return batchNormalization(\n          input, mean, variance, offset, scale, this.epsilon);\n    });\n  }\n\n  getConfig(): serialization.ConfigDict {\n    const config: serialization.ConfigDict = {\n      axis: this.axis,\n      epsilon: this.epsilon,\n      center: this.center,\n      scale: this.scale,\n      betaInitializer: serializeInitializer(this.betaInitializer),\n      gammaInitializer: serializeInitializer(this.gammaInitializer),\n      betaRegularizer: serializeRegularizer(this.betaRegularizer),\n      gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n    };\n    const baseConfig = super.getConfig();\n    Object.assign(config, baseConfig);\n    return config;\n  }\n}\nserialization.registerClass(LayerNormalization);\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}